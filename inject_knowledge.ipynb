{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea25fe-5955-4d13-8056-78714181ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, 'nlp_architect/models/aspect_extraction_with_kg')\n",
    "import absa_utils\n",
    "from absa_utils import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ac1e7-d9f1-4bef-9bc0-57851c0dbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'nlp_architect/models/aspect_extraction_with_kg/data/csv/' # includes the re-labeled devices dataset\n",
    "# data_path = 'nlp_architect/models/aspect_extraction_with_kg/data/csv_original/' # use this path for the original devices dataset\n",
    "out_path = 'nlp_architect/models/aspect_extraction_with_kg/data/knowledge_injected/'\n",
    "domains = ['laptops','restaurants','device']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d367c8-bb00-47f9-b0eb-321ad09518e6",
   "metadata": {},
   "source": [
    "### Load domain-specific KGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf821dc5-a345-4b64-86ff-d383820c8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_terms = pickle.load(open('seed_terms.pkl', 'rb'))\n",
    "seed_dist = pickle.load(open('seed_dist.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee41b0-bb45-4c32-80eb-727a8ef58b5f",
   "metadata": {},
   "source": [
    "### Insert pivot tokens into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92a394-f404-4f5b-9a66-a43184299f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_df(example: InputExample) -> pd.DataFrame:\n",
    "    header =['TOKEN','LABEL','HEAD','HEAD_WORD','DEP_REL','POS','SUB_TOKENS']\n",
    "    df= pd.DataFrame(columns=header)\n",
    "    df['TOKEN'] = example.words\n",
    "    df['LABEL'] = example.labels\n",
    "    df['HEAD']=example.heads\n",
    "    df['HEAD_WORD']=example.head_words\n",
    "    df['DEP_REL']=example.syn_rels\n",
    "    df['POS']=example.pos_tags\n",
    "    df['SUB_TOKENS']=[' '.join(el) if len(el)>1 else '' for el in example.sub_toks]\n",
    "    \n",
    "    return df.astype(str)\n",
    "\n",
    "def concate_mw_pivot_phrase(df: pd.DataFrame, indices: List[int]) -> None:\n",
    "    df2 = df.copy()\n",
    "    increment = 0\n",
    "    for item in indices:\n",
    "        index = item[0]\n",
    "        index = index + increment\n",
    "        line = pd.DataFrame({'TOKEN': [item[1]],\n",
    "                             'LABEL': ['O'],\n",
    "                             'HEAD': [df2.iloc[index-1]['HEAD']],\n",
    "                             'HEAD_WORD': [df2.iloc[index-1]['HEAD_WORD']],\n",
    "                             'DEP_REL': [df2.iloc[index-1]['DEP_REL']],\n",
    "                             'POS': [df2.iloc[index-1]['POS']],\n",
    "                             'SUB_TOKENS': [df2.iloc[index-1]['SUB_TOKENS']]\n",
    "                            \n",
    "                            },\n",
    "                            \n",
    "                            index=[index])\n",
    "        df2 = pd.concat([df2.iloc[:index], line, df2.iloc[index:]]).reset_index(drop=True)\n",
    "        increment+=1\n",
    "        \n",
    "    return df2\n",
    "\n",
    "def get_cn_mw_pivot_phrase_indices(df: pd.DataFrame, candidate_pos: List[str], seed_dist, pvb, pvi, kg_pct) -> List[int]:\n",
    "    indices_for_pivot_phrase = []\n",
    "    seed_dist = seed_dist.copy()\n",
    "    for j in range(len(seed_dist)):\n",
    "        seed_dist[j] = list(itertools.chain.from_iterable([seed_dist[j][i] for i in seed_dist[j].keys()]))\n",
    "    seed_dist = list(itertools.chain.from_iterable(seed_dist))\n",
    "    if kg_pct < 1:\n",
    "        np.random.shuffle(seed_dist)\n",
    "        seed_dist = seed_dist[:int(len(seed_dist)*kg_pct)]\n",
    "    \n",
    "    row_iterator = df.iterrows()\n",
    "    index, last = next(row_iterator) # take first item from row_iterator\n",
    "    compound_start = None\n",
    "    amod_start = None\n",
    "    for i, row in row_iterator:\n",
    "        asp_candidate = None\n",
    "        if last['DEP_REL'] == 'compound':\n",
    "            if compound_start is None:\n",
    "                compound_start = index\n",
    "        elif compound_start is not None:\n",
    "            asp_candidate = ' '.join(df[compound_start:i]['TOKEN'].values)\n",
    "        elif last['DEP_REL'] == 'amod':\n",
    "            if amod_start is None:\n",
    "                amod_start = index\n",
    "        elif (last['POS'] in candidate_pos):\n",
    "            if amod_start is None:\n",
    "                asp_candidate =  last['TOKEN']\n",
    "            else:\n",
    "                asp_candidate = ' '.join(df[amod_start:i]['TOKEN'].values)\n",
    "        else:\n",
    "            amod_start = None\n",
    "\n",
    "        if asp_candidate is not None:\n",
    "            path_found = False\n",
    "            for k in range(len(asp_candidate.split())):\n",
    "                if ' '.join(asp_candidate.split()[k:]) in seed_dist:\n",
    "                    path_found = True\n",
    "                    break\n",
    "            if path_found:\n",
    "                if compound_start is not None or amod_start is not None:\n",
    "                    if amod_start is not None:\n",
    "                        amod_start += k\n",
    "                    mw_start = [s for s in [compound_start, amod_start] if s is not None][0]\n",
    "                    indices_for_pivot_phrase.append([mw_start+1, pvb])\n",
    "                    indices_for_pivot_phrase += [[k, pvi] for k in range(mw_start+2, i+1)]\n",
    "                else:\n",
    "                    indices_for_pivot_phrase.append([i, pvb])\n",
    "            compound_start = None\n",
    "            amod_start = None\n",
    "        last = row\n",
    "        index = i\n",
    "        \n",
    "    return indices_for_pivot_phrase\n",
    "\n",
    "def get_stochastic_mw_gold_label_pivot_phrase_indices(df, r, fpr, pvb, pvi, candidate_pos):\n",
    "    indices_for_pivot_phrase = []\n",
    "    row_iterator = df.iterrows()\n",
    "    index, last = next(row_iterator) # take first item from row_iterator\n",
    "    urand_p = np.random.uniform()\n",
    "    for i, row in row_iterator:\n",
    "        if last['LABEL'] == 'B-ASP' and urand_p <= r:\n",
    "            indices_for_pivot_phrase.append([i,pvb])\n",
    "        elif last['LABEL'] == 'I-ASP' and urand_p <= r:\n",
    "            indices_for_pivot_phrase.append([i,pvi])\n",
    "        elif np.random.uniform() <= fpr:\n",
    "            if np.random.uniform() <= 0.5:\n",
    "                indices_for_pivot_phrase.append([i,pvb])\n",
    "            else:\n",
    "                indices_for_pivot_phrase.append([i,pvi])\n",
    "        last = row\n",
    "        index = i\n",
    "        \n",
    "    return indices_for_pivot_phrase\n",
    "\n",
    "def write_example_cn_pivotPhrase_to_file(examples: List[InputExample], file: str, candidate_pos: List[str], seed_dist, domain, mode, p, r, pvb, pvi, pred_dict, prob_thresh, kg_pct) -> None:\n",
    "    \n",
    "    if mode == 'train':\n",
    "        df_all = pd.concat([example_to_df(e) for e in examples])\n",
    "        num_pos = df_all['LABEL'].value_counts()[['B-ASP','I-ASP']].sum()\n",
    "        num_neg = df_all['LABEL'].value_counts().sum() - num_pos\n",
    "        tp = r * num_pos\n",
    "        fp = (tp / p) - tp\n",
    "        fpr = fp / num_neg\n",
    "    \n",
    "    with open(file, 'w',  encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['TOKEN', 'LABEL', 'HEAD', 'HEAD_WORD', 'DEP_REL', 'POS', 'SUB_TOKENS'])\n",
    "        \n",
    "        for i in range(len(examples)):\n",
    "            example = examples[i]\n",
    "            df = example_to_df(example)\n",
    "            df['TOKEN'] = df['TOKEN'].str.lower()\n",
    "            if mode == 'train':\n",
    "                indices = get_stochastic_mw_gold_label_pivot_phrase_indices(df, r, fpr, pvb, pvi, candidate_pos)\n",
    "            else:\n",
    "                if pred_dict is None or pred_dict['prob'][i] > prob_thresh:\n",
    "                    indices = get_cn_mw_pivot_phrase_indices(df, candidate_pos, seed_dist, pvb, pvi, kg_pct)\n",
    "                else:\n",
    "                    indices = []\n",
    "            df = concate_mw_pivot_phrase(df, indices)\n",
    "            for index, row in df.iterrows():\n",
    "                writer.writerow(row.values)\n",
    "            writer.writerow(['_'] * 7)\n",
    "            writer.writerow(['_'] * 7)\n",
    "            \n",
    "def score_mw_pivot_insertion_accuracy(examples, pvb, pvi):\n",
    "    tp = []; fn = []; fp = []; tn = []\n",
    "    \n",
    "    for example in examples:\n",
    "        example_df = example_to_df(example)\n",
    "        for i in range(example_df.shape[0]):\n",
    "            token = example_df.iloc[i, example_df.columns == 'TOKEN'].values[0]\n",
    "            pos = example_df.iloc[i, example_df.columns == 'POS'].values[0]\n",
    "            dep_rel = example_df.iloc[i, example_df.columns == 'DEP_REL'].values[0]\n",
    "            pos_dep_rel = pos + '_' + dep_rel\n",
    "            label = example_df.iloc[i, example_df.columns == 'LABEL'].values[0]\n",
    "            prediction = example_df.iloc[min(i+1, example_df.shape[0]-1), example_df.columns == 'TOKEN'].values[0]\n",
    "            if label == 'B-ASP':\n",
    "                if prediction == pvb:\n",
    "                    tp.append([token, example_df, pos, dep_rel, pos_dep_rel, label, prediction])\n",
    "                else:\n",
    "                    fn.append([token, example_df, pos, dep_rel, pos_dep_rel, label, prediction])\n",
    "            elif label == 'I-ASP':\n",
    "                if prediction == pvi:\n",
    "                    tp.append([token, example_df, pos, dep_rel, pos_dep_rel, label, prediction])\n",
    "                else:\n",
    "                    fn.append([token, example_df, pos, dep_rel, pos_dep_rel, label, prediction])\n",
    "            else:\n",
    "                if prediction in [pvb, pvi]:\n",
    "                    fp.append([token, example_df, pos, dep_rel, pos_dep_rel, label, prediction])\n",
    "                else:\n",
    "                    tn.append([token, example_df, pos, dep_rel, pos_dep_rel, label, prediction])\n",
    "\n",
    "    precision = len(tp) / (len(tp) + len(fp))\n",
    "    recall = len(tp) / (len(tp) + len(fn))\n",
    "    f1 = len(tp) / (len(tp) + 0.5*(len(fp) + len(fn)))\n",
    "    \n",
    "    return [precision, recall, f1, len(tp), len(fp), len(fn), len(tn)], {'tp' : tp, 'fp' : fp, 'fn' : fn, 'tn' : tn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1976a8-f6c1-47d3-90a1-c2ef4b7d0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "kg_pct_test = 1\n",
    "candidate_pos = ['NN', 'NNS', 'NNP']\n",
    "\n",
    "for model_name in ['bert', 'deberta']:\n",
    "    if model_name == 'bert':\n",
    "        pvb = 'reltodomainb'\n",
    "        pvi = 'reltodomaini'\n",
    "    elif model_name == 'deberta':\n",
    "        pvb = '--------------------------------'\n",
    "        pvi = '----------------------------------------------------------------'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    for source in domains:\n",
    "        for target in set(domains).difference([source]):\n",
    "            for split in range(1, 4):\n",
    "                in_folder = os.path.join(source + '_to_' + target + '_' + str(split))\n",
    "                out_folder = os.path.join(model_name, in_folder)\n",
    "                Path(os.path.join(out_path, out_folder)).mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Prepare the dev dataset\n",
    "                examples = absa_utils.read_examples_from_file(data_dir=(os.path.join(data_path, in_folder)), mode='dev')\n",
    "                write_example_cn_pivotPhrase_to_file(examples, os.path.join(out_path, out_folder, 'dev.csv'), candidate_pos, \n",
    "                                                     seed_dist[target], domain = target, mode='dev', p = None, \n",
    "                                                     r = None, pvb = pvb, pvi = pvi, pred_dict = None, prob_thresh = None, kg_pct = 1)\n",
    "                \n",
    "                # Prepare the test dataset\n",
    "                examples = absa_utils.read_examples_from_file(data_dir=(os.path.join(data_path, in_folder)), mode='test')\n",
    "                write_example_cn_pivotPhrase_to_file(examples, os.path.join(out_path, out_folder, 'test.csv'), candidate_pos, \n",
    "                                                     seed_dist[target], domain = target, mode='test', p = None, \n",
    "                                                     r = None, pvb = pvb, pvi = pvi, pred_dict = None, prob_thresh = None, kg_pct = kg_pct_test)\n",
    "                \n",
    "                # Estimate precision and recall of knowledge injection on the dev dataset\n",
    "                examples = absa_utils.read_examples_from_file(data_dir=os.path.join(out_path, out_folder), mode='dev')\n",
    "                results, examples_dict = score_mw_pivot_insertion_accuracy(examples, pvb, pvi)\n",
    "                results = pd.DataFrame(results).transpose()\n",
    "                results.columns = ['precision', 'recall', 'f1', 'tp', 'fp', 'fn', 'tn']\n",
    "                \n",
    "                # Prepare the train dataset\n",
    "                examples = absa_utils.read_examples_from_file(data_dir=(os.path.join(data_path, in_folder)), mode='train')\n",
    "                write_example_cn_pivotPhrase_to_file(examples, os.path.join(out_path, out_folder, 'train.csv'), candidate_pos, \n",
    "                                                     seed_dist[source], domain = source, mode='train', p = results['precision'][0], \n",
    "                                                     r = results['recall'][0], pvb = pvb, pvi = pvi, pred_dict = None, prob_thresh = None, kg_pct = 1)\n",
    "\n",
    "    copyfile(os.path.join(data_path, 'labels.txt'), os.path.join(out_path, model_name, 'labels.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
