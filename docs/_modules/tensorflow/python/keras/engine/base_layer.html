

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.keras.engine.base_layer &mdash; NLP Architect by Intel® AI Lab 0.4.post2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html">
          

          
            
            <img src="../../../../../_static/nlp_architect_logo_white.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../main.html">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials.html">Jupyter Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../developer_guide.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../service.html">REST Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../model_zoo.html">Model Zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">NLP/NLU Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../absa.html">Aspect Based Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../chunker.html">Sequence Chunker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ner_crf.html">Named Entity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../intent.html">Intent Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../np_segmentation.html">Noun Phrase Semantic Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bist_parser.html">BIST Dependency Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../word_sense.html">Most Common Word Sense</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../np2vec.html">Noun Phrase to Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../supervised_sentiment.html">Supervised Sentiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../reading_comprehension.html">Reading Comprehension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../memn2n.html">End-to-End Memory Networks for Goal Oriented Dialogue</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tcn.html">TCN Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../crosslingual_emb.html">Unsupervised Crosslingual Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cross_doc_coref.html">Cross Document Co-Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../identifying_semantic_relation.html">Semantic Relation Identification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sparse_gnmt.html">Sparse Neural Machine Translation</a></li>
</ul>
<p class="caption"><span class="caption-text">Solutions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../absa_solution.html">Aspect Based Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../term_set_expansion.html">Set Expansion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../trend_analysis.html">Trend Analysis</a></li>
</ul>
<p class="caption"><span class="caption-text">Pipelines</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../spacy_bist.html">Spacy-BIST Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../spacy_np_annotator.html">Spacy-NP Annotator</a></li>
</ul>
<p class="caption"><span class="caption-text">For Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api.html">API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">NLP Architect by Intel® AI Lab</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.keras.engine.base_layer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.keras.engine.base_layer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="c1"># pylint: disable=protected-access</span>
<span class="sd">&quot;&quot;&quot;Contains the base Layer class, from which all layers inherit.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">inspect</span>  <span class="c1"># Necessary supplement to tf_inspect to deal with variadic args.</span>
<span class="kn">import</span> <span class="nn">itertools</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">zip</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="k">import</span> <span class="n">node_def_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">autograph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.distribute</span> <span class="k">import</span> <span class="n">distribution_strategy_context</span> <span class="k">as</span> <span class="n">ds_context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.distribute</span> <span class="k">import</span> <span class="n">values</span> <span class="k">as</span> <span class="n">distribute_values</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">execute</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">function</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">auto_control_deps</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">func_graph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_util</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">backend</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">constraints</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">initializers</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="k">import</span> <span class="n">base_layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="k">import</span> <span class="n">input_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.mixed_precision.experimental</span> <span class="k">import</span> <span class="n">autocast_variable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.mixed_precision.experimental</span> <span class="k">import</span> <span class="n">policy</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="k">import</span> <span class="n">generic_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="k">import</span> <span class="n">tf_utils</span>
<span class="c1"># A module that only depends on `keras.layers` import these from here.</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils.generic_utils</span> <span class="k">import</span> <span class="n">to_snake_case</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils.tf_utils</span> <span class="k">import</span> <span class="n">is_tensor_or_tensor_list</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.module</span> <span class="k">import</span> <span class="n">module</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">variables</span> <span class="k">as</span> <span class="n">tf_variables</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">data_structures</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">layer_utils</span> <span class="k">as</span> <span class="n">trackable_layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">object_identity</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">tracking</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">function_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_decorator</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_inspect</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.tf_export</span> <span class="k">import</span> <span class="n">keras_export</span>
<span class="kn">from</span> <span class="nn">tensorflow.tools.docs</span> <span class="k">import</span> <span class="n">doc_controls</span>

<span class="c1"># Prefix that is added to the TF op layer names.</span>
<span class="n">_TF_OP_LAYER_NAME_PREFIX</span> <span class="o">=</span> <span class="s1">&#39;tf_op_layer_&#39;</span>


<span class="nd">@keras_export</span><span class="p">(</span><span class="s1">&#39;keras.layers.Layer&#39;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base layer class.</span>

<span class="sd">  This is the class from which all layers inherit.</span>

<span class="sd">  A layer is a class implementing common neural networks operations, such</span>
<span class="sd">  as convolution, batch norm, etc. These operations require managing weights,</span>
<span class="sd">  losses, updates, and inter-layer connectivity.</span>

<span class="sd">  Users will just instantiate a layer and then treat it as a callable.</span>

<span class="sd">  We recommend that descendants of `Layer` implement the following methods:</span>

<span class="sd">  * `__init__()`: Save configuration in member variables</span>
<span class="sd">  * `build()`: Called once from `__call__`, when we know the shapes of inputs</span>
<span class="sd">    and `dtype`. Should have the calls to `add_weight()`, and then</span>
<span class="sd">    call the super&#39;s `build()` (which sets `self.built = True`, which is</span>
<span class="sd">    nice in case the user wants to call `build()` manually before the</span>
<span class="sd">    first `__call__`).</span>
<span class="sd">  * `call()`: Called in `__call__` after making sure `build()` has been called</span>
<span class="sd">    once. Should actually perform the logic of applying the layer to the</span>
<span class="sd">    input tensors (which should be passed in as the first argument).</span>

<span class="sd">  Arguments:</span>
<span class="sd">    trainable: Boolean, whether the layer&#39;s variables should be trainable.</span>
<span class="sd">    name: String name of the layer.</span>
<span class="sd">    dtype: Default dtype of the layer&#39;s weights (default of `None` means use the</span>
<span class="sd">      type of the first input).</span>
<span class="sd">    dynamic: Set this to `True` if your layer should only be run eagerly, and</span>
<span class="sd">      should not be used to generate a static computation graph.</span>
<span class="sd">      This would be the case for a Tree-RNN or a recursive network,</span>
<span class="sd">      for example, or generally for any layer that manipulates tensors</span>
<span class="sd">      using Python control flow. If `False`, we assume that the layer can</span>
<span class="sd">      safely be used to generate a static computation graph.</span>

<span class="sd">  Read-only properties:</span>
<span class="sd">    name: The name of the layer (string).</span>
<span class="sd">    dtype: Default dtype of the layer&#39;s weights (default of `None` means use the</span>
<span class="sd">      type of the first input).</span>
<span class="sd">    updates: List of update ops of this layer.</span>
<span class="sd">    losses: List of losses added by this layer.</span>
<span class="sd">    trainable_weights: List of variables to be included in backprop.</span>
<span class="sd">    non_trainable_weights: List of variables that should not be</span>
<span class="sd">      included in backprop.</span>
<span class="sd">    weights: The concatenation of the lists trainable_weights and</span>
<span class="sd">      non_trainable_weights (in this order).</span>

<span class="sd">  Mutable properties:</span>
<span class="sd">    trainable: Whether the layer should be trained (boolean).</span>
<span class="sd">    input_spec: Optional (list of) `InputSpec` object(s) specifying the</span>
<span class="sd">      constraints on inputs that can be accepted by the layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># See tf.Module for the usage of this property.</span>
  <span class="c1"># The key for _obj_reference_counts_dict is a Trackable, which could be a</span>
  <span class="c1"># variable or layer etc. tf.Module._flatten will fail to flatten the key</span>
  <span class="c1"># since it is trying to convert Trackable to a string. This attribute can be</span>
  <span class="c1"># ignored even after the fix of nest lib, since the trackable object should</span>
  <span class="c1"># already been available as individual attributes. _obj_reference_counts_dict</span>
  <span class="c1"># just contains a copy of them.</span>
  <span class="n">_TF_MODULE_IGNORED_PROPERTIES</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
      <span class="p">(</span><span class="s1">&#39;_obj_reference_counts_dict&#39;</span><span class="p">,),</span>
      <span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="n">_TF_MODULE_IGNORED_PROPERTIES</span>
  <span class="p">))</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># These properties should be set by the user via keyword arguments.</span>
    <span class="c1"># note that &#39;dtype&#39;, &#39;input_shape&#39; and &#39;batch_input_shape&#39;</span>
    <span class="c1"># are only applicable to input layers: do not pass these keywords</span>
    <span class="c1"># to non-input layers.</span>
    <span class="n">allowed_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;input_shape&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_input_shape&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weights&#39;</span><span class="p">,</span>
        <span class="s1">&#39;activity_regularizer&#39;</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="c1"># Validate optional keyword arguments.</span>
    <span class="n">generic_utils</span><span class="o">.</span><span class="n">validate_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">allowed_kwargs</span><span class="p">)</span>

    <span class="c1"># Mutable properties</span>
    <span class="c1"># Indicates whether the layer&#39;s weights are updated during training</span>
    <span class="c1"># and whether the layer&#39;s updates are run during training.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="c1"># A stateful layer is a layer whose updates are run during inference too,</span>
    <span class="c1"># for instance stateful RNNs.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stateful</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Indicates whether `build` needs to be called upon layer call, to create</span>
    <span class="c1"># the layer&#39;s weights.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Provides information about which inputs are compatible with the layer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_init_set_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;activity_regularizer&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_updates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of zero-argument lambdas which return Tensors, used for variable</span>
    <span class="c1"># regularizers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of symbolic Tensors containing activity regularizers and losses</span>
    <span class="c1"># manually added through `add_loss` in graph-building mode.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of loss values containing activity regularizers and losses</span>
    <span class="c1"># manually added through `add_loss` during eager execution. It is cleared</span>
    <span class="c1"># after every batch.</span>
    <span class="c1"># Because we plan on eventually allowing a same model instance to be trained</span>
    <span class="c1"># in eager mode or graph mode alternatively, we need to keep track of</span>
    <span class="c1"># eager losses and symbolic losses via separate attributes.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of metric instances corresponding to the symbolic metric tensors</span>
    <span class="c1"># added using the `add_metric` API.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># TODO(psv): Remove this property.</span>
    <span class="c1"># A dictionary that maps metric names to metric result tensors. The results</span>
    <span class="c1"># are the running averages of metric values over an epoch.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics_tensors</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_set_dtype_and_policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_call_convention</span> <span class="o">=</span> <span class="p">(</span><span class="n">base_layer_utils</span>
                             <span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span><span class="p">)</span>
    <span class="c1"># Dependencies tracked via attribute assignment.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="p">[])</span>

    <span class="c1"># These lists will be filled via successive calls</span>
    <span class="c1"># to self._add_inbound_node().</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">call_argspec</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span> <span class="o">=</span> <span class="s1">&#39;training&#39;</span> <span class="ow">in</span> <span class="n">call_argspec</span><span class="o">.</span><span class="n">args</span>

    <span class="c1"># Whether the `call` method can be used to build a TF graph without issues.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>

    <span class="c1"># Manage input shape information if passed.</span>
    <span class="k">if</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">or</span> <span class="s1">&#39;batch_input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="c1"># In this case we will later create an input layer</span>
      <span class="c1"># to insert before the current layer</span>
      <span class="k">if</span> <span class="s1">&#39;batch_input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;batch_input_shape&#39;</span><span class="p">])</span>
      <span class="k">elif</span> <span class="s1">&#39;input_shape&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;batch_size&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">batch_input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;input_shape&#39;</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batch_input_shape</span> <span class="o">=</span> <span class="n">batch_input_shape</span>

    <span class="c1"># Manage initial weight values if passed.</span>
    <span class="k">if</span> <span class="s1">&#39;weights&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;weights&#39;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates the variables of the layer (optional, for subclass implementers).</span>

<span class="sd">    This is a method that implementers of subclasses of `Layer` or `Model`</span>
<span class="sd">    can override if they need a state-creation step in-between</span>
<span class="sd">    layer instantiation and layer call.</span>

<span class="sd">    This is typically used to create the weights of `Layer` subclasses.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      input_shape: Instance of `TensorShape`, or list of instances of</span>
<span class="sd">        `TensorShape` if the layer expects a list of inputs</span>
<span class="sd">        (one instance per input).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
    <span class="sd">&quot;&quot;&quot;This is where the layer&#39;s logic lives.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs: Input tensor, or list/tuple of input tensors.</span>
<span class="sd">        **kwargs: Additional keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor or list/tuple of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inputs</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">partitioner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">synchronization</span><span class="o">=</span><span class="n">tf_variables</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
                 <span class="n">aggregation</span><span class="o">=</span><span class="n">tf_variables</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a new variable to the layer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      name: Variable name.</span>
<span class="sd">      shape: Variable shape. Defaults to scalar if unspecified.</span>
<span class="sd">      dtype: The type of the variable. Defaults to `self.dtype` or `float32`.</span>
<span class="sd">      initializer: initializer instance (callable).</span>
<span class="sd">      regularizer: regularizer instance (callable).</span>
<span class="sd">      trainable: whether the variable should be part of the layer&#39;s</span>
<span class="sd">        &quot;trainable_variables&quot; (e.g. variables, biases)</span>
<span class="sd">        or &quot;non_trainable_variables&quot; (e.g. BatchNorm mean, stddev).</span>
<span class="sd">        Note, if the current variable scope is marked as non-trainable</span>
<span class="sd">        then this parameter is ignored and any added variables are also</span>
<span class="sd">        marked as non-trainable. `trainable` defaults to `True` unless</span>
<span class="sd">        `synchronization` is set to `ON_READ`.</span>
<span class="sd">      constraint: constraint instance (callable).</span>
<span class="sd">      partitioner: Partitioner to be passed to the `Trackable` API.</span>
<span class="sd">      use_resource: Whether to use `ResourceVariable`.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize. If `synchronization` is set to `ON_READ`,</span>
<span class="sd">        `trainable` must not be set to `True`.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      **kwargs: Additional keyword arguments. Accepted values are `getter` and</span>
<span class="sd">        `collections`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The created variable.  Usually either a `Variable` or `ResourceVariable`</span>
<span class="sd">      instance.  If `partitioner` is not `None`, a `PartitionedVariable`</span>
<span class="sd">      instance is returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called with partioned variable regularization and</span>
<span class="sd">        eager execution is enabled.</span>
<span class="sd">      ValueError: When giving unsupported dtype and no initializer or when</span>
<span class="sd">        trainable has been set to True with synchronization set as `ON_READ`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">shape</span> <span class="o">=</span> <span class="p">()</span>
    <span class="c1"># Validate optional keyword arguments.</span>
    <span class="k">for</span> <span class="n">kwarg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">kwarg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;getter&#39;</span><span class="p">,</span> <span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="s1">&#39;experimental_autocast&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword argument:&#39;</span><span class="p">,</span> <span class="n">kwarg</span><span class="p">)</span>
    <span class="n">getter</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;getter&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">collections_arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate an</span>
    <span class="c1"># AutoCastVariable should never be created.</span>
    <span class="n">autocast</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">backend</span><span class="o">.</span><span class="n">floatx</span><span class="p">()</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="o">.</span><span class="n">name</span>
    <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>
    <span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span>
    <span class="n">constraint</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">synchronization</span> <span class="o">==</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">ON_READ</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;Synchronization value can be set to &#39;</span>
            <span class="s1">&#39;VariableSynchronization.ON_READ only for non-trainable variables. &#39;</span>
            <span class="s1">&#39;You have specified trainable=True and &#39;</span>
            <span class="s1">&#39;synchronization=VariableSynchronization.ON_READ.&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Set trainable to be false when variable is to be synced on read.</span>
        <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">elif</span> <span class="n">trainable</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Initialize variable when no initializer provided</span>
    <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span>
      <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_floating</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">()</span>
      <span class="c1"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span>
      <span class="c1"># If dtype is DT_BOOL, provide a default value `FALSE`</span>
      <span class="k">elif</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_integer</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_unsigned</span> <span class="ow">or</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_bool</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">zeros</span><span class="p">()</span>
      <span class="c1"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;An initializer for variable </span><span class="si">%s</span><span class="s1"> of type </span><span class="si">%s</span><span class="s1"> is required&#39;</span>
                         <span class="s1">&#39; for layer </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="n">variable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
        <span class="c1"># TODO(allenl): a `make_variable` equivalent should be added as a</span>
        <span class="c1"># `Trackable` method.</span>
        <span class="n">getter</span><span class="o">=</span><span class="n">getter</span> <span class="ow">or</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">make_variable</span><span class="p">,</span>
        <span class="c1"># Manage errors in Layer rather than Trackable.</span>
        <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="n">partitioner</span><span class="p">,</span>
        <span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span>
        <span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>
    <span class="n">backend</span><span class="o">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">autocast</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span><span class="o">.</span><span class="n">should_cast_variables</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">distribute_values</span><span class="o">.</span><span class="n">DistributedVariable</span><span class="p">):</span>
        <span class="n">variable</span> <span class="o">=</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">AutoCastDistributedVariable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">variable</span> <span class="o">=</span> <span class="n">autocast_variable</span><span class="o">.</span><span class="n">AutoCastVariable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># TODO(fchollet): in the future, this should be handled at the</span>
      <span class="c1"># level of variable creation, and weight regularization losses</span>
      <span class="c1"># should be variable attributes.</span>
      <span class="n">name_in_scope</span> <span class="o">=</span> <span class="n">variable</span><span class="o">.</span><span class="n">name</span><span class="p">[:</span><span class="n">variable</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span><span class="n">name_in_scope</span><span class="p">,</span>
                                         <span class="n">variable</span><span class="p">,</span>
                                         <span class="n">regularizer</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">variable</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the config of the layer.</span>

<span class="sd">    A layer config is a Python dictionary (serializable)</span>
<span class="sd">    containing the configuration of a layer.</span>
<span class="sd">    The same layer can be reinstantiated later</span>
<span class="sd">    (without its trained weights) from this configuration.</span>

<span class="sd">    The config of a layer does not include connectivity</span>
<span class="sd">    information, nor the layer class name. These are handled</span>
<span class="sd">    by `Network` (one layer of abstraction above).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Python dictionary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;trainable&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_batch_input_shape&#39;</span><span class="p">):</span>
      <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_input_shape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_input_shape</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">):</span>
      <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>
    <span class="c1"># TODO(reedwm): Handle serializing self._mixed_precision_policy.</span>
    <span class="k">return</span> <span class="n">config</span>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a layer from its config.</span>

<span class="sd">    This method is the reverse of `get_config`,</span>
<span class="sd">    capable of instantiating the same layer from the config</span>
<span class="sd">    dictionary. It does not handle layer connectivity</span>
<span class="sd">    (handled by Network), nor weights (handled by `set_weights`).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        config: A Python dictionary, typically the</span>
<span class="sd">            output of get_config.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A layer instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="sd">    Assumes that the layer will be built</span>
<span class="sd">    to match that input shape provided.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input_shape: Shape tuple (tuple of integers)</span>
<span class="sd">            or list of shape tuples (one per output tensor of the layer).</span>
<span class="sd">            Shape tuples can include None for free dimensions,</span>
<span class="sd">            instead of an integer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An input shape tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># In this case we build the model first in order to do shape inference.</span>
      <span class="c1"># This is acceptable because the framework only calls</span>
      <span class="c1"># `compute_output_shape` on shape values that the layer would later be</span>
      <span class="c1"># built for. It would however cause issues in case a user attempts to</span>
      <span class="c1"># use `compute_output_shape` manually (these users will have to</span>
      <span class="c1"># implement `compute_output_shape` themselves).</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">graph_mode</span><span class="p">():</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">(</span><span class="s1">&#39;graph&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
          <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
          <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
              <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">generate_placeholders_from_shape</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
          <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span><span class="p">:</span>
              <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
          <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;We could not automatically infer &#39;</span>
                                      <span class="s1">&#39;the static shape of the layer</span><span class="se">\&#39;</span><span class="s1">s output.&#39;</span>
                                      <span class="s1">&#39; Please implement the &#39;</span>
                                      <span class="s1">&#39;`compute_output_shape` method on your &#39;</span>
                                      <span class="s1">&#39;layer (</span><span class="si">%s</span><span class="s1">).&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=unused-argument</span>
    <span class="sd">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs: Tensor or list of tensors.</span>
<span class="sd">        mask: Tensor or list of tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None or a tensor (or list of tensors,</span>
<span class="sd">            one per output tensor of the layer).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39; does not support masking, &#39;</span>
                        <span class="s1">&#39;but was passed an input_mask: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
      <span class="c1"># masking not explicitly supported: return None as mask.</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="c1"># if masking is explicitly supported, by default</span>
    <span class="c1"># carry over the input mask</span>
    <span class="k">return</span> <span class="n">mask</span>

  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps `call`, applying pre- and post-processing steps.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: input tensor(s).</span>
<span class="sd">      *args: additional positional arguments to be passed to `self.call`.</span>
<span class="sd">      **kwargs: additional keyword arguments to be passed to `self.call`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor(s).</span>

<span class="sd">    Note:</span>
<span class="sd">      - The following optional keyword arguments are reserved for specific uses:</span>
<span class="sd">        * `training`: Boolean scalar tensor of Python boolean indicating</span>
<span class="sd">          whether the `call` is meant for training or inference.</span>
<span class="sd">        * `mask`: Boolean input mask.</span>
<span class="sd">      - If the layer&#39;s `call` method takes a `mask` argument (as some Keras</span>
<span class="sd">        layers do), its default value will be set to the mask generated</span>
<span class="sd">        for `inputs` by the previous layer (if `input` did come from</span>
<span class="sd">        a layer that generated a corresponding mask, i.e. if it came from</span>
<span class="sd">        a Keras layer with masking support.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the layer&#39;s `call` method returns None (an invalid value).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># Accept NumPy inputs by converting to Tensors.</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">):</span>
      <span class="c1"># Don&#39;t call `ops.convert_to_tensor` on all `inputs` because</span>
      <span class="c1"># `SparseTensors` can&#39;t be converted to `Tensor`.</span>
      <span class="k">def</span> <span class="nf">_convert_non_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
          <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

      <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_convert_non_tensor</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># We will attempt to build a TF graph if &amp; only if all inputs are symbolic.</span>
    <span class="c1"># This is always the case in graph mode. It can also be the case in eager</span>
    <span class="c1"># mode when all inputs can be traced back to `keras.Input()` (when building</span>
    <span class="c1"># models using the functional API).</span>
    <span class="n">build_graph</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">are_all_symbolic_tensors</span><span class="p">(</span><span class="n">input_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">build_graph</span><span class="p">:</span>
      <span class="c1"># Only create Keras history if at least one tensor originates from a</span>
      <span class="c1"># `keras.Input`. Otherwise this Layer may be being used outside the Keras</span>
      <span class="c1"># framework.</span>
      <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">needs_keras_history</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_keras_history</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Handle Keras mask propagation from previous layer to current layer.</span>
    <span class="n">previous_mask</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_compute_mask</span><span class="p">:</span>
      <span class="n">previous_mask</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">collect_previous_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
      <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;mask&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span> <span class="ow">and</span> <span class="s1">&#39;mask&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span>
          <span class="ow">not</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">is_all_none</span><span class="p">(</span><span class="n">previous_mask</span><span class="p">)):</span>
        <span class="c1"># The previous layer generated a mask, and mask was not explicitly</span>
        <span class="c1"># pass to __call__, hence we set previous_mask as the default value.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;mask&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">previous_mask</span>

    <span class="c1"># Clear eager losses on top level model call.</span>
    <span class="c1"># We are clearing the losses only on the top level model call and not on</span>
    <span class="c1"># every layer/mode call because layer/model may be reused.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_eager_or_tf_function</span><span class="p">()</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_call_context</span><span class="p">()):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_clear_losses</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">call_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">build_graph</span><span class="p">):</span>
      <span class="c1"># Check input assumptions set after layer building, e.g. input shape.</span>
      <span class="k">if</span> <span class="n">build_graph</span><span class="p">:</span>
        <span class="c1"># Symbolic execution on symbolic tensors. We will attempt to build</span>
        <span class="c1"># the corresponding TF subgraph inside `backend.get_graph()`</span>
        <span class="n">input_spec</span><span class="o">.</span><span class="n">assert_input_compatibility</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">(),</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="p">()):</span>
          <span class="c1"># Build layer if applicable (if the `build` method has been</span>
          <span class="c1"># overridden).</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

          <span class="c1"># Wrapping `call` function in autograph to allow for dynamic control</span>
          <span class="c1"># dependencies in call. We are limiting this to subclassed layers as</span>
          <span class="c1"># autograph is strictly needed only for subclassed layers.</span>
          <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_subclassed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">decorators</span><span class="p">,</span> <span class="n">original_func</span> <span class="o">=</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">unwrap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
            <span class="n">converted_func</span> <span class="o">=</span> <span class="n">autograph</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">original_func</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">decorators</span><span class="p">:</span>
              <span class="n">call_fn</span> <span class="o">=</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">rewrap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="n">original_func</span><span class="p">,</span>
                                            <span class="n">converted_func</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">call_fn</span> <span class="o">=</span> <span class="n">converted_func</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">call_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span>

          <span class="c1"># Explicitly pass the learning phase placeholder to `call` if</span>
          <span class="c1"># the `training` argument was left unspecified by the user.</span>
          <span class="c1"># This behavior is restricted to the managed Keras FuncGraph.</span>
          <span class="c1"># TODO(omalleyt): Reconcile this with new `trainable` behavior</span>
          <span class="c1"># when available.</span>
          <span class="n">learning_phase_passed_by_framework</span> <span class="o">=</span> <span class="kc">False</span>
          <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span> <span class="ow">and</span>
              <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">training_arg_passed_to_call</span><span class="p">(</span>
                  <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="ow">and</span>
              <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_keras_graph</span><span class="p">()):</span>
            <span class="n">learning_phase_passed_by_framework</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;training&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">learning_phase</span><span class="p">()</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
              <span class="k">with</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">autocast_context_manager</span><span class="p">(</span>
                  <span class="n">input_list</span><span class="p">,</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span><span class="o">.</span><span class="n">should_cast_variables</span><span class="p">):</span>
                <span class="c1"># Add auto_control_deps in V2 when they are not already added by</span>
                <span class="c1"># a `tf.function`.</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">()</span> <span class="ow">and</span>
                    <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_eager_or_tf_function</span><span class="p">()):</span>
                  <span class="k">with</span> <span class="n">auto_control_deps</span><span class="o">.</span><span class="n">AutomaticControlDependencies</span><span class="p">()</span> <span class="k">as</span> <span class="n">acd</span><span class="p">:</span>
                    <span class="n">outputs</span> <span class="o">=</span> <span class="n">call_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                    <span class="c1"># Wrap Tensors in `outputs` in `tf.identity` to avoid</span>
                    <span class="c1"># circular dependencies.</span>
                    <span class="n">outputs</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">mark_as_return</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">acd</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                  <span class="n">outputs</span> <span class="o">=</span> <span class="n">call_fn</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
              <span class="n">exception_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
              <span class="n">exception_msg</span> <span class="o">=</span> <span class="s1">&#39;Tensor objects are only iterable when eager&#39;</span>
              <span class="k">if</span> <span class="n">exception_msg</span> <span class="ow">in</span> <span class="n">exception_str</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;You are attempting to use Python control &#39;</span>
                                <span class="s1">&#39;flow in a layer that was not declared to be &#39;</span>
                                <span class="s1">&#39;dynamic. Pass `dynamic=True` to the class &#39;</span>
                                <span class="s1">&#39;constructor.</span><span class="se">\n</span><span class="s1">Encountered error:</span><span class="se">\n</span><span class="s1">&quot;&quot;&quot;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span>
                                <span class="n">exception_str</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&quot;&quot;&quot;&#39;</span><span class="p">)</span>
              <span class="k">raise</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We will use static shape inference to return symbolic tensors</span>
            <span class="c1"># matching the specifications of the layer outputs.</span>
            <span class="c1"># Since `self.dynamic` is True, we will never attempt to</span>
            <span class="c1"># run the underlying TF graph (which is disconnected).</span>
            <span class="c1"># TODO(fchollet): consider py_func as an alternative, which</span>
            <span class="c1"># would enable us to run the underlying graph if needed.</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_symbolic_call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

          <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;A layer</span><span class="se">\&#39;</span><span class="s1">s `call` method should return a &#39;</span>
                             <span class="s1">&#39;Tensor or a list of Tensors, not None &#39;</span>
                             <span class="s1">&#39;(layer: &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;).&#39;</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">have_all_keras_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">learning_phase_passed_by_framework</span><span class="p">:</span>
              <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_connectivity_metadata_</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle_activity_regularization</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">)</span>
          <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_set_inputs&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># Subclassed network: explicitly set metadata normally set by</span>
            <span class="c1"># a call to self._set_inputs().</span>
            <span class="c1"># TODO(b/120997007): This should be done in Eager as well, but</span>
            <span class="c1"># causes garbage collection issues because of the placeholders</span>
            <span class="c1"># created on the default Keras graph.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Eager execution on data tensors.</span>
        <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name_scope</span><span class="p">()):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
          <span class="k">with</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">autocast_context_manager</span><span class="p">(</span>
              <span class="n">input_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span><span class="o">.</span><span class="n">should_cast_variables</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle_activity_regularization</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_metadata</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># Optionally load weight values specified at layer instantiation.</span>
      <span class="c1"># TODO(fchollet): consider enabling this with eager execution too.</span>
      <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_initial_weights&#39;</span><span class="p">)</span> <span class="ow">and</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span><span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_weights</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dynamic</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span>

  <span class="nd">@trainable</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="p">[]):</span>
      <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">activity_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optional regularizer function for the output of this layer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span>

  <span class="nd">@activity_regularizer</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">activity_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optional regularizer function for the output of this layer.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span> <span class="o">=</span> <span class="n">regularizer</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
      <span class="n">nested</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;trainable_weights&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">+</span> <span class="n">nested</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[]</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">non_trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
      <span class="n">nested</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;non_trainable_weights&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="o">+</span> <span class="n">nested</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">nested</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">)</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="o">+</span> <span class="n">nested</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of all layer variables/weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_weights</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">updates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateful</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="n">updates</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_updates</span><span class="p">:</span>
        <span class="c1"># Filter out updates created in a cross-replica context when in a</span>
        <span class="c1"># replica context and vice versa.</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="s1">&#39;_in_cross_replica_context&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="o">!=</span>
            <span class="n">ds_context</span><span class="o">.</span><span class="n">in_cross_replica_context</span><span class="p">()):</span>
          <span class="k">continue</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
          <span class="k">try</span><span class="p">:</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">()</span>
          <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;Trying to capture a tensor from an inner function&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
              <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">check_graph_consistency</span><span class="p">(</span>
                  <span class="n">method</span><span class="o">=</span><span class="s1">&#39;add_update&#39;</span><span class="p">,</span> <span class="n">force_raise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">raise</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">check_graph_consistency</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;add_update&#39;</span><span class="p">)</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">updates</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Losses which are associated with this `Layer`.</span>

<span class="sd">    Variable regularization tensors are created when this property is accessed,</span>
<span class="sd">    so it is eager safe: accessing `losses` under a `tf.GradientTape` will</span>
<span class="sd">    propagate gradients back to the corresponding variables.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">collected_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># If any eager losses are present, we assume the model to be part of an</span>
    <span class="c1"># eager training loop (either a custom one or the one used when</span>
    <span class="c1"># `run_eagerly=True`), and so we always return just the eager losses in that</span>
    <span class="c1"># case.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span><span class="p">:</span>
      <span class="n">collected_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">collected_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">regularizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span><span class="p">:</span>
      <span class="n">loss_tensor</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">loss_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">collected_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">collected_losses</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;losses&#39;</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="sd">    Some losses (for instance, activity regularization losses) may be dependent</span>
<span class="sd">    on the inputs passed when calling a layer. Hence, when reusing the same</span>
<span class="sd">    layer on different inputs `a` and `b`, some entries in `layer.losses` may</span>
<span class="sd">    be dependent on `a` and some on `b`. This method automatically keeps track</span>
<span class="sd">    of dependencies.</span>

<span class="sd">    This method can be used inside a subclassed layer or model&#39;s `call`</span>
<span class="sd">    function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    class MyLayer(tf.keras.layers.Layer):</span>
<span class="sd">      def call(inputs, self):</span>
<span class="sd">        self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)</span>
<span class="sd">        return inputs</span>
<span class="sd">    ```</span>

<span class="sd">    This method can also be called directly on a Functional Model during</span>
<span class="sd">    construction. In this case, any loss Tensors passed to this Model must</span>
<span class="sd">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>
<span class="sd">    losses become part of the model&#39;s topology and are tracked in `get_config`.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    x = tf.keras.layers.Dense(10)(inputs)</span>
<span class="sd">    outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    # Actvity regularization.</span>
<span class="sd">    model.add_loss(tf.abs(tf.reduce_mean(x)))</span>
<span class="sd">    ```</span>

<span class="sd">    If this is not the case for your loss (if, for example, your loss references</span>
<span class="sd">    a `Variable` of one of the model&#39;s layers), you can wrap your loss in a</span>
<span class="sd">    zero-argument lambda. These losses are not tracked as part of the model&#39;s</span>
<span class="sd">    topology since they can&#39;t be serialized.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    inputs = tf.keras.Input(shape=(10,))</span>
<span class="sd">    x = tf.keras.layers.Dense(10)(inputs)</span>
<span class="sd">    outputs = tf.keras.layers.Dense(1)(x)</span>
<span class="sd">    model = tf.keras.Model(inputs, outputs)</span>
<span class="sd">    # Weight regularization.</span>
<span class="sd">    model.add_loss(lambda: tf.reduce_mean(x.kernel))</span>
<span class="sd">    ```</span>

<span class="sd">    The `get_losses_for` method allows to retrieve the losses relevant to a</span>
<span class="sd">    specific set of inputs.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses</span>
<span class="sd">        may also be zero-argument callables which create a loss tensor.</span>
<span class="sd">      inputs: Ignored when executing eagerly. If anything other than None is</span>
<span class="sd">        passed, it signals the losses are conditional on some of the layer&#39;s</span>
<span class="sd">        inputs, and thus they should only be run where these inputs are</span>
<span class="sd">        available. This is the case for activity regularization losses, for</span>
<span class="sd">        instance. If `None` is passed, the losses are assumed</span>
<span class="sd">        to be unconditional, and will apply across all dataflows of the layer</span>
<span class="sd">        (e.g. weight regularization losses).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_tag_unconditional</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Will be filtered out when computing the .losses property</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">floatx</span><span class="p">())</span>
      <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">check_graph_consistency</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;add_loss&#39;</span><span class="p">)</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">_unconditional_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="k">return</span> <span class="n">loss</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

    <span class="n">callable_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">symbolic_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">callable_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_tag_unconditional</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">floatx</span><span class="p">())</span>
      <span class="c1"># TF Functions should take the eager path.</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">and</span>
          <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_tf_function</span><span class="p">()):</span>
        <span class="n">symbolic_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_tag_unconditional</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">tensor_util</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
        <span class="n">eager_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_tag_unconditional</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span> <span class="o">+=</span> <span class="n">callable_losses</span>

    <span class="n">call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_call_context</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">eager_losses</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">call_context</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Expected a symbolic Tensors or a callable for the loss value. &#39;</span>
          <span class="s1">&#39;Please wrap your loss computation in a zero argument `lambda`.&#39;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">+=</span> <span class="n">eager_losses</span>

    <span class="k">if</span> <span class="n">call_context</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="ow">in</span> <span class="n">symbolic_losses</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="ow">in</span> <span class="n">symbolic_losses</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
          <span class="n">new_layers</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_keras_history</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>
          <span class="c1"># Losses must be keyed on inputs no matter what in order to</span>
          <span class="c1"># be supported in DistributionStrategy.</span>
          <span class="n">add_loss_layer</span> <span class="o">=</span> <span class="n">AddLoss</span><span class="p">(</span><span class="n">unconditional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
          <span class="n">add_loss_layer</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>
          <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">add_loss_layer</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_insert_layers</span><span class="p">(</span><span class="n">new_layers</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbolic_loss</span><span class="p">)</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_clear_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Used every step in eager to reset losses.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">filter_empty_layer_containers</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">):</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">_clear_losses</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gather_children_attribute</span><span class="p">(</span><span class="s1">&#39;metrics&#39;</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds metric tensor to the layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: Metric tensor.</span>
<span class="sd">      aggregation: Sample-wise metric reduction function. If `aggregation=None`,</span>
<span class="sd">        it indicates that the metric tensor provided has been aggregated</span>
<span class="sd">        already. eg, `bin_acc = BinaryAccuracy(name=&#39;acc&#39;)` followed by</span>
<span class="sd">        `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation=&#39;mean&#39;, the</span>
<span class="sd">        given metric tensor will be sample-wise reduced using `mean` function.</span>
<span class="sd">        eg, `model.add_metric(tf.reduce_sum(outputs), name=&#39;output_mean&#39;,</span>
<span class="sd">        aggregation=&#39;mean&#39;)`.</span>
<span class="sd">      name: String metric name.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `aggregation` is anything other than None or `mean`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">aggregation</span> <span class="o">!=</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;We currently support only `mean` sample-wise metric aggregation. &#39;</span>
          <span class="s1">&#39;You provided aggregation=`</span><span class="si">%s</span><span class="s1">`&#39;</span> <span class="o">%</span> <span class="n">aggregation</span><span class="p">)</span>

    <span class="n">from_metric_obj</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_metric_obj&#39;</span><span class="p">)</span>
    <span class="n">is_symbolic</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">call_context</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_call_context</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">from_metric_obj</span><span class="p">:</span>
      <span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x), aggregation=&#39;mean&#39;)`</span>
      <span class="c1"># In eager mode, we use metric name to lookup a metric. Without a name,</span>
      <span class="c1"># a new Mean metric wrapper will be created on every model/layer call.</span>
      <span class="c1"># So, we raise an error when no name is provided.</span>
      <span class="c1"># We will do the same for symbolic mode for consistency although a name</span>
      <span class="c1"># will be generated if no name is provided.</span>

      <span class="c1"># We will not raise this error in the foll use case for the sake of</span>
      <span class="c1"># consistency as name in provided in the metric constructor.</span>
      <span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span>
      <span class="c1"># model.add_metric(mean(outputs))</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please provide a name for your metric like &#39;</span>
                       <span class="s1">&#39;`self.add_metric(tf.reduce_sum(inputs), &#39;</span>
                       <span class="s1">&#39;name=</span><span class="se">\&#39;</span><span class="s1">mean_activation</span><span class="se">\&#39;</span><span class="s1">, aggregation=</span><span class="se">\&#39;</span><span class="s1">mean</span><span class="se">\&#39;</span><span class="s1">)`&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">call_context</span><span class="p">:</span>
      <span class="c1"># TF Function path should take the eager path.</span>
      <span class="k">if</span> <span class="n">is_symbolic</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_tf_function</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_symbolic_add_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eager_add_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">is_symbolic</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected a symbolic Tensor for the metric value, &#39;</span>
                         <span class="s1">&#39;received: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

      <span class="c1"># Possible a metric was added in a Layer&#39;s `build`.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_symbolic_add_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">return</span>

      <span class="k">if</span> <span class="n">from_metric_obj</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Using the result of calling a `Metric` object &#39;</span>
                         <span class="s1">&#39;when calling `add_metric` on a Functional &#39;</span>
                         <span class="s1">&#39;Model is not supported. Please pass the &#39;</span>
                         <span class="s1">&#39;Tensor to monitor directly.&#39;</span><span class="p">)</span>

      <span class="c1"># Insert layers into the Keras Graph Network.</span>
      <span class="n">new_layers</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_keras_history</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">add_metric_layer</span> <span class="o">=</span> <span class="n">AddMetric</span><span class="p">(</span><span class="n">aggregation</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
      <span class="n">add_metric_layer</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">new_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">add_metric_layer</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_insert_layers</span><span class="p">(</span><span class="n">new_layers</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add update op(s), potentially dependent on layer inputs.</span>

<span class="sd">    Weight updates (for instance, the updates of the moving mean and variance</span>
<span class="sd">    in a BatchNormalization layer) may be dependent on the inputs passed</span>
<span class="sd">    when calling a layer. Hence, when reusing the same layer on</span>
<span class="sd">    different inputs `a` and `b`, some entries in `layer.updates` may be</span>
<span class="sd">    dependent on `a` and some on `b`. This method automatically keeps track</span>
<span class="sd">    of dependencies.</span>

<span class="sd">    The `get_updates_for` method allows to retrieve the updates relevant to a</span>
<span class="sd">    specific set of inputs.</span>

<span class="sd">    This call is ignored when eager execution is enabled (in that case, variable</span>
<span class="sd">    updates are run on the fly and thus do not need to be tracked for later</span>
<span class="sd">    execution).</span>

<span class="sd">    Arguments:</span>
<span class="sd">      updates: Update op, or list/tuple of update ops, or zero-arg callable</span>
<span class="sd">        that returns an update op. A zero-arg callable should be passed in</span>
<span class="sd">        order to disable running the updates by setting `trainable=False`</span>
<span class="sd">        on this Layer, when executing in Eager mode.</span>
<span class="sd">      inputs: If anything other than None is passed, it signals the updates</span>
<span class="sd">        are conditional on some of the layer&#39;s inputs,</span>
<span class="sd">        and thus they should only be run where these inputs are available.</span>
<span class="sd">        This is the case for BatchNormalization updates, for instance.</span>
<span class="sd">        If None, the updates will be taken into account unconditionally,</span>
<span class="sd">        and you are responsible for making sure that any dependency they might</span>
<span class="sd">        have is available at runtime.</span>
<span class="sd">        A step counter might fall into this category.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">to_list</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span>

    <span class="c1"># All updates can be run immediately in Eager or in a tf.function.</span>
    <span class="k">if</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_eager_or_tf_function</span><span class="p">():</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_frozen_context</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">update</span> <span class="ow">in</span> <span class="n">updates</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">update</span><span class="p">):</span>
            <span class="n">update</span><span class="p">()</span>
      <span class="k">return</span>

    <span class="k">def</span> <span class="nf">process_update</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Standardize update ops.</span>

<span class="sd">      Arguments:</span>
<span class="sd">        x: Tensor, op, or callable.</span>

<span class="sd">      Returns:</span>
<span class="sd">        An update op.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">update</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">process_update</span><span class="p">(</span><span class="n">x</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">():</span>
          <span class="c1"># In V1 mode, call the callable right away and process. This is needed</span>
          <span class="c1"># for TPU strategy.</span>
          <span class="k">return</span> <span class="n">update</span><span class="p">()</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Operation</span><span class="p">):</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">x</span>
      <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;op&#39;</span><span class="p">):</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">op</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">update</span><span class="o">.</span><span class="n">_unconditional_update</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
      <span class="n">update</span><span class="o">.</span><span class="n">_in_cross_replica_context</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">ds_context</span><span class="o">.</span><span class="n">has_strategy</span><span class="p">()</span> <span class="ow">and</span> <span class="n">ds_context</span><span class="o">.</span><span class="n">in_cross_replica_context</span><span class="p">())</span>
      <span class="k">return</span> <span class="n">update</span>

    <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span><span class="n">process_update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">updates</span><span class="p">]</span>
    <span class="c1"># Non-callable Updates are run automatically inside `call` in V2, so</span>
    <span class="c1"># they do not need to be tracked later.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">executing_eagerly_outside_functions</span><span class="p">()</span> <span class="ow">and</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">is_in_call_context</span><span class="p">()):</span>
      <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">updates</span> <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">u</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_updates</span> <span class="o">+=</span> <span class="n">updates</span>

  <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the weights of the layer, from Numpy arrays.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        weights: a list of Numpy arrays. The number</span>
<span class="sd">            of arrays and their shape must match</span>
<span class="sd">            number of the dimensions of the weights</span>
<span class="sd">            of the layer (i.e. it should match the</span>
<span class="sd">            output of `get_weights`).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the provided weights list does not match the</span>
<span class="sd">            layer&#39;s specifications.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You called `set_weights(weights)` on layer &quot;&#39;</span> <span class="o">+</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot; with a  weight list of length &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39;, but the layer was expecting &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; weights. Provided weights: &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;...&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="n">weight_value_tuples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">param_values</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pv</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param_values</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">pv</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Layer weight shape &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">pv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span>
                         <span class="s1">&#39; not compatible with &#39;</span>
                         <span class="s1">&#39;provided weight shape &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
      <span class="n">weight_value_tuples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
    <span class="n">backend</span><span class="o">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current weights of the layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Weights values as a list of numpy arrays.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
    <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">batch_get_value</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_updates_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves updates relevant to a specific set of inputs.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of update ops of the layer that depend on `inputs`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Requesting unconditional updates.</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">updates</span> <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">_unconditional_update</span><span class="p">]</span>

    <span class="c1"># Requesting input-conditional updates.</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">updates</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">u</span><span class="o">.</span><span class="n">_unconditional_update</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">reachable</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_reachable_from_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">updates</span> <span class="k">if</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">reachable</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">get_losses_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves losses relevant to a specific set of inputs.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of loss tensors of the layer that depend on `inputs`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Requesting unconditional losses.</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="k">if</span> <span class="n">l</span><span class="o">.</span><span class="n">_unconditional_loss</span><span class="p">]</span>

    <span class="c1"># Requesting input-conditional losses.</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">l</span><span class="o">.</span><span class="n">_unconditional_loss</span><span class="p">]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">reachable</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">get_reachable_from_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">losses</span> <span class="k">if</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">reachable</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A mask tensor</span>
<span class="sd">        (or list of tensors if the layer has multiple inputs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A mask tensor</span>
<span class="sd">        (or list of tensors if the layer has multiple outputs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one inbound node,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Input mask tensor (potentially None) or list of input</span>
<span class="sd">        mask tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer is connected to</span>
<span class="sd">        more than one incoming layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one inbound node,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output mask tensor (potentially None) or list of output</span>
<span class="sd">        mask tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer is connected to</span>
<span class="sd">        more than one incoming layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
      <span class="k">return</span> <span class="p">[</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A shape tuple</span>
<span class="sd">        (or list of shape tuples if the layer has multiple inputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;input_shapes&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;input shape&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A shape tuple</span>
<span class="sd">        (or list of shape tuples if the layer has multiple outputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;output_shapes&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;output shape&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;input&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer, index of the node</span>
<span class="sd">            from which to retrieve the attribute.</span>
<span class="sd">            E.g. `node_index=0` will correspond to the</span>
<span class="sd">            first time the layer was called.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span>
                                             <span class="s1">&#39;output&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one input,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Input tensor or list of input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer is connected to</span>
<span class="sd">        more than one incoming layers.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If called in Eager mode.</span>
<span class="sd">      AttributeError: If no inbound nodes are found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                           <span class="s1">&#39; is not connected, no input to return.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one output,</span>
<span class="sd">    i.e. if it is connected to one incoming layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor or list of output tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      AttributeError: if the layer is connected to more than one incoming</span>
<span class="sd">        layers.</span>
<span class="sd">      RuntimeError: if called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39; has no inbound nodes.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span> <span class="s1">&#39;output&#39;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the input shape(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has exactly one input,</span>
<span class="sd">    i.e. if it is connected to one incoming layer, or if all inputs</span>
<span class="sd">    have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Input shape, as an integer shape tuple</span>
<span class="sd">        (or list of shape tuples, one tuple per input tensor).</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer has no defined input_shape.</span>
<span class="sd">        RuntimeError: if called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer has never been called &#39;</span>
                           <span class="s1">&#39;and thus has no defined input shape.&#39;</span><span class="p">)</span>
    <span class="n">all_input_shapes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">input_shapes</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_input_shapes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">input_shapes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer &quot;&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="o">+</span>
                           <span class="s1">&#39; has multiple inbound nodes, &#39;</span>
                           <span class="s1">&#39;with different input shapes. Hence &#39;</span>
                           <span class="s1">&#39;the notion of &quot;input shape&quot; is &#39;</span>
                           <span class="s1">&#39;ill-defined for the layer. &#39;</span>
                           <span class="s1">&#39;Use `get_input_shape_at(node_index)` &#39;</span>
                           <span class="s1">&#39;instead.&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Count the total number of scalars composing the weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An integer count.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if the layer isn&#39;t yet built</span>
<span class="sd">          (in which case its weights aren&#39;t yet defined).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;Sequential&#39;</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>  <span class="c1"># pylint: disable=no-value-for-parameter</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You tried to call `count_params` on &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                         <span class="s1">&#39;, but the layer isn</span><span class="se">\&#39;</span><span class="s1">t built. &#39;</span>
                         <span class="s1">&#39;You can build it manually via: `&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                         <span class="s1">&#39;.build(batch_input_shape)`.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves the output shape(s) of a layer.</span>

<span class="sd">    Only applicable if the layer has one output,</span>
<span class="sd">    or if all outputs have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output shape, as an integer shape tuple</span>
<span class="sd">        (or list of shape tuples, one tuple per output tensor).</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: if the layer has no defined output shape.</span>
<span class="sd">        RuntimeError: if called in Eager mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer has never been called &#39;</span>
                           <span class="s1">&#39;and thus has no defined output shape.&#39;</span><span class="p">)</span>
    <span class="n">all_output_shapes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
        <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">output_shapes</span><span class="p">)</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_output_shapes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output_shapes</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;The layer &quot;</span><span class="si">%s</span><span class="s1">&quot;&#39;</span>
                           <span class="s1">&#39; has multiple inbound nodes, &#39;</span>
                           <span class="s1">&#39;with different output shapes. Hence &#39;</span>
                           <span class="s1">&#39;the notion of &quot;output shape&quot; is &#39;</span>
                           <span class="s1">&#39;ill-defined for the layer. &#39;</span>
                           <span class="s1">&#39;Use `get_output_shape_at(node_index)` &#39;</span>
                           <span class="s1">&#39;instead.&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">inbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use! Only for compatibility with external Keras.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span>

  <span class="nd">@property</span>
  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">do_not_doc_inheritable</span>
  <span class="k">def</span> <span class="nf">outbound_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Deprecated, do NOT use! Only for compatibility with external Keras.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes</span>

  <span class="c1">##############################################################################</span>
  <span class="c1"># Methods &amp; attributes below are public aliases of other methods.            #</span>
  <span class="c1">##############################################################################</span>

  <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply the layer on a input.</span>

<span class="sd">    This is an alias of `self.__call__`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">      inputs: Input tensor(s).</span>
<span class="sd">      *args: additional positional arguments to be passed to `self.call`.</span>
<span class="sd">      **kwargs: additional keyword arguments to be passed to `self.call`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Output tensor(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="nd">@doc_controls</span><span class="o">.</span><span class="n">for_subclass_implementers</span>
  <span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Alias for `add_weight`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of all layer variables/weights.</span>

<span class="sd">    Alias of `self.weights`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">non_trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_weights</span>

  <span class="c1">##############################################################################</span>
  <span class="c1"># Methods &amp; attributes below are all private and only used by the framework. #</span>
  <span class="c1">##############################################################################</span>

  <span class="k">def</span> <span class="nf">_set_dtype_and_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets self._dtype and self._mixed_precision_policy.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span><span class="o">.</span><span class="n">default_variable_dtype</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If a non-policy dtype is passed, no casting should be done. So we use</span>
        <span class="c1"># the &quot;infer&quot; policy, which does no casting.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="s1">&#39;infer&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">global_policy</span><span class="p">()</span>
      <span class="c1"># If the global policy has not been set, it will be an &quot;infer&quot; policy</span>
      <span class="c1"># without a default variable dtype, and so self._dtype will be None. In</span>
      <span class="c1"># that case, self._dtype will be set when the layer is built or called.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span><span class="o">.</span><span class="n">default_variable_dtype</span>

  <span class="k">def</span> <span class="nf">_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>

  <span class="k">def</span> <span class="nf">_init_set_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">zero_based</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">unique_object_name</span><span class="p">(</span>
          <span class="n">generic_utils</span><span class="o">.</span><span class="n">to_snake_case</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span>
          <span class="n">zero_based</span><span class="o">=</span><span class="n">zero_based</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>

  <span class="k">def</span> <span class="nf">_get_existing_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">match</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">name</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">match</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">&#39;Please provide different names for the metrics you have added. &#39;</span>
          <span class="s1">&#39;We found </span><span class="si">{}</span><span class="s1"> metrics with the name: &quot;</span><span class="si">{}</span><span class="s1">&quot;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">match</span><span class="p">),</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">match</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_eager_add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># If the given metric is available in `metrics` list we just update state</span>
    <span class="c1"># on it, otherwise we create a new metric instance and</span>
    <span class="c1"># add it to the `metrics` list.</span>
    <span class="n">metric_obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_metric_obj&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">metric_obj</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="n">metric_obj</span><span class="o">.</span><span class="n">name</span>

    <span class="n">match</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
      <span class="c1"># Tensors that come from a Metric object already updated the Metric state.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">metric_obj</span><span class="p">:</span>
        <span class="n">match</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">metric_obj</span><span class="p">:</span>
      <span class="k">assert</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
      <span class="n">metric_obj</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_mean_metric</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_symbolic_add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">check_graph_consistency</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;add_metric&#39;</span><span class="p">)</span>
    <span class="n">match</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">aggregation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Iterate over the metrics and check if the given metric exists already.</span>
      <span class="c1"># This can happen when a metric instance is created in subclassed model</span>
      <span class="c1"># layer `__init__` and we have tracked that instance already in</span>
      <span class="c1"># model.__setattr__.</span>
      <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">result_tensor</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">metric_obj</span> <span class="o">=</span> <span class="n">match</span>
      <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_metric_obj&#39;</span><span class="p">):</span>
        <span class="c1"># We track the instance using the metadata on the result tensor.</span>
        <span class="n">result_tensor</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">metric_obj</span> <span class="o">=</span> <span class="n">result_tensor</span><span class="o">.</span><span class="n">_metric_obj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;We do not support adding an aggregated metric result tensor that &#39;</span>
            <span class="s1">&#39;is not the output of a `tf.keras.metrics.Metric` metric instance. &#39;</span>
            <span class="s1">&#39;Without having access to the metric instance we cannot reset the &#39;</span>
            <span class="s1">&#39;state of a metric after every epoch during training. You can &#39;</span>
            <span class="s1">&#39;create a `tf.keras.metrics.Metric` instance and pass the result &#39;</span>
            <span class="s1">&#39;here or pass an un-aggregated result with `aggregation` parameter &#39;</span>
            <span class="s1">&#39;set as `mean`. For example: `self.add_metric(tf.reduce_sum(inputs)&#39;</span>
            <span class="s1">&#39;, name=</span><span class="se">\&#39;</span><span class="s1">mean_activation</span><span class="se">\&#39;</span><span class="s1">, aggregation=</span><span class="se">\&#39;</span><span class="s1">mean</span><span class="se">\&#39;</span><span class="s1">)`&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># If a non-aggregated tensor is given as input (ie. `aggregation` is</span>
      <span class="c1"># explicitly set to `mean`), we wrap the tensor in `Mean` metric.</span>
      <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">result_tensor</span> <span class="o">=</span> <span class="n">match</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">metric_obj</span> <span class="o">=</span> <span class="n">match</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">metric_obj</span><span class="p">,</span> <span class="n">result_tensor</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_mean_metric</span><span class="p">(</span>
            <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics_tensors</span><span class="p">[</span><span class="n">metric_obj</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_tensor</span>

  <span class="k">def</span> <span class="nf">_handle_weight_regularization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">regularizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create lambdas which compute regularization losses.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_loss_for_variable</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Creates a regularization loss `Tensor` for variable `v`.&quot;&quot;&quot;</span>
      <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/Regularizer&#39;</span><span class="p">):</span>
        <span class="n">regularization</span> <span class="o">=</span> <span class="n">regularizer</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">regularization</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">PartitionedVariable</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">variable</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_loss_for_variable</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_loss_for_variable</span><span class="p">,</span> <span class="n">variable</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_handle_activity_regularization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># Apply activity regularization.</span>
    <span class="c1"># Note that it should be applied every time the layer creates a new</span>
    <span class="c1"># output, since it is output-specific.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span><span class="p">:</span>
      <span class="n">output_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s1">&#39;ActivityRegularizer&#39;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">output_list</span><span class="p">:</span>
          <span class="n">activity_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
              <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">activity_loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
          <span class="c1"># Make activity regularization strength batch-agnostic.</span>
          <span class="n">mean_activity_loss</span> <span class="o">=</span> <span class="n">activity_loss</span> <span class="o">/</span> <span class="n">batch_size</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">mean_activity_loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_mask_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">):</span>
    <span class="n">flat_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">mask_already_computed</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_compute_output_and_mask_jointly&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">all</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">))</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">mask_already_computed</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;compute_mask&#39;</span><span class="p">):</span>
        <span class="n">output_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">)</span>
        <span class="c1"># `compute_mask` can return a single `None` even when a Layer</span>
        <span class="c1"># has multiple outputs.</span>
        <span class="k">if</span> <span class="n">output_masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="n">flat_masks</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">flat_masks</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_masks</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">flat_masks</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">]</span>

      <span class="k">for</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">,</span> <span class="n">flat_masks</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">output</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="n">mask</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
          <span class="c1"># C Type such as np.ndarray.</span>
          <span class="k">pass</span>

    <span class="k">if</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">are_all_symbolic_tensors</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">flat_outputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="c1"># Do not track masks for `TensorFlowOpLayer` construction.</span>
          <span class="n">output</span><span class="o">.</span><span class="n">_keras_mask</span><span class="o">.</span><span class="n">_keras_history_checked</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_set_connectivity_metadata_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">call_convention</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_call_convention&#39;</span><span class="p">,</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">call_convention</span> <span class="o">==</span> <span class="p">(</span><span class="n">base_layer_utils</span>
                             <span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s1">&#39;This layer (&quot;</span><span class="si">{}</span><span class="s1">&quot;) takes an `inputs` argument in `call()`, &#39;</span>
            <span class="s1">&#39;and only the `inputs` argument may be specified as a positional &#39;</span>
            <span class="s1">&#39;argument. Pass everything else as a keyword argument &#39;</span>
            <span class="s1">&#39;(those arguments will not be tracked &#39;</span>
            <span class="s1">&#39;as inputs to the layer).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">call_convention</span> <span class="o">==</span> <span class="p">(</span><span class="n">base_layer_utils</span>
                               <span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">SINGLE_POSITIONAL_ARGUMENT</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s1">&#39;This layer (&quot;</span><span class="si">{}</span><span class="s1">&quot;) takes a single positional argument in `call()`,&#39;</span>
            <span class="s1">&#39; which is by convention the `inputs` argument, &#39;</span>
            <span class="s1">&#39;and only this argument may be specified as a positional argument. &#39;</span>
            <span class="s1">&#39;Pass everything else as a keyword argument &#39;</span>
            <span class="s1">&#39;(those arguments will not be tracked &#39;</span>
            <span class="s1">&#39;as inputs to the layer).&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="c1"># If the layer returns tensors from its inputs, unmodified,</span>
    <span class="c1"># we copy them to avoid loss of tensor metadata.</span>
    <span class="n">output_ls</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">inputs_ls</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">output_ls_copy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_ls</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs_ls</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">output_ls_copy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_ls_copy</span><span class="p">)</span>

    <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inputs_from_call_args</span><span class="p">(</span>
        <span class="n">call_args</span><span class="o">=</span><span class="p">(</span><span class="n">inputs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">args</span><span class="p">,</span> <span class="n">call_kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># Add an inbound node to the layer, so it can keep track of this call.</span>
    <span class="c1"># This updates the layer history of the output tensor(s).</span>
    <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># `mask` should not be serialized.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_add_inbound_node</span><span class="p">(</span>
        <span class="n">input_tensors</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output_tensors</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">arguments</span><span class="o">=</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span>

  <span class="k">def</span> <span class="nf">_inputs_from_call_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call_args</span><span class="p">,</span> <span class="n">call_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get Layer inputs from __call__ *args and **kwargs.</span>

<span class="sd">    Args:</span>
<span class="sd">      call_args: The positional arguments passed to __call__.</span>
<span class="sd">      call_kwargs: The keyword argument dict passed to __call__.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple of (inputs, non_input_kwargs). These may be the same objects as</span>
<span class="sd">      were passed in (call_args and call_kwargs).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">call_convention</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_call_convention&#39;</span><span class="p">,</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">call_convention</span> <span class="ow">in</span> <span class="p">(</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span><span class="p">,</span>
        <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">SINGLE_POSITIONAL_ARGUMENT</span><span class="p">)):</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># TypeError raised earlier in __call__.</span>
      <span class="k">return</span> <span class="n">call_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">call_kwargs</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">call_arg_spec</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
      <span class="c1"># There is no explicit &quot;inputs&quot; argument expected or provided to</span>
      <span class="c1"># call(). Arguments which have default values are considered non-inputs,</span>
      <span class="c1"># and arguments without are considered inputs.</span>
      <span class="k">if</span> <span class="n">call_arg_spec</span><span class="o">.</span><span class="n">defaults</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">call_arg_spec</span><span class="o">.</span><span class="n">varargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
              <span class="s1">&#39;Layers may not accept both positional arguments and &#39;</span>
              <span class="s1">&#39;arguments with default values (unable to determine which &#39;</span>
              <span class="s1">&#39;are inputs to the layer). &#39;</span>
              <span class="s1">&#39;Issue occurred with layer &quot;</span><span class="si">%s</span><span class="s1">&quot;&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="n">keyword_arg_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="n">call_arg_spec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">call_arg_spec</span><span class="o">.</span><span class="n">defaults</span><span class="p">):])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">keyword_arg_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="c1"># Training is never an input argument name, to allow signatures like</span>
        <span class="c1"># call(x, training).</span>
      <span class="n">keyword_arg_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">unwrapped_call</span> <span class="o">=</span> <span class="n">tf_decorator</span><span class="o">.</span><span class="n">unwrap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
      <span class="n">bound_args</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getcallargs</span><span class="p">(</span>
          <span class="n">unwrapped_call</span><span class="p">,</span> <span class="o">*</span><span class="n">call_args</span><span class="p">,</span> <span class="o">**</span><span class="n">call_kwargs</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">call_arg_spec</span><span class="o">.</span><span class="n">varkw</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">var_kwargs</span> <span class="o">=</span> <span class="n">bound_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">call_arg_spec</span><span class="o">.</span><span class="n">varkw</span><span class="p">)</span>
        <span class="n">bound_args</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">var_kwargs</span><span class="p">)</span>
        <span class="n">keyword_arg_names</span> <span class="o">=</span> <span class="n">keyword_arg_names</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">var_kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
      <span class="n">all_args</span> <span class="o">=</span> <span class="n">call_arg_spec</span><span class="o">.</span><span class="n">args</span>
      <span class="k">if</span> <span class="n">all_args</span> <span class="ow">and</span> <span class="n">bound_args</span><span class="p">[</span><span class="n">all_args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="ow">is</span> <span class="bp">self</span><span class="p">:</span>
        <span class="c1"># Ignore the &#39;self&#39; argument of methods</span>
        <span class="n">bound_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">call_arg_spec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">all_args</span> <span class="o">=</span> <span class="n">all_args</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
      <span class="n">non_input_arg_values</span> <span class="o">=</span> <span class="p">{}</span>
      <span class="n">input_arg_values</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">remaining_args_are_keyword</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">for</span> <span class="n">argument_name</span> <span class="ow">in</span> <span class="n">all_args</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">argument_name</span> <span class="ow">in</span> <span class="n">keyword_arg_names</span><span class="p">:</span>
          <span class="n">remaining_args_are_keyword</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">remaining_args_are_keyword</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s1">&#39;Found a positional argument in a layer call after a non-input &#39;</span>
                <span class="s1">&#39;argument. All arguments after &quot;training&quot; must be keyword &#39;</span>
                <span class="s1">&#39;arguments, and are not tracked as inputs to the layer. &#39;</span>
                <span class="s1">&#39;Issue occurred with layer &quot;</span><span class="si">%s</span><span class="s1">&quot;&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">remaining_args_are_keyword</span><span class="p">:</span>
          <span class="n">non_input_arg_values</span><span class="p">[</span><span class="n">argument_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">bound_args</span><span class="p">[</span><span class="n">argument_name</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">input_arg_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bound_args</span><span class="p">[</span><span class="n">argument_name</span><span class="p">])</span>
      <span class="k">if</span> <span class="n">call_arg_spec</span><span class="o">.</span><span class="n">varargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_arg_values</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">bound_args</span><span class="p">[</span><span class="n">call_arg_spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">])</span>
      <span class="k">return</span> <span class="n">input_arg_values</span><span class="p">,</span> <span class="n">non_input_arg_values</span>

  <span class="k">def</span> <span class="nf">_add_inbound_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">input_tensors</span><span class="p">,</span>
                        <span class="n">output_tensors</span><span class="p">,</span>
                        <span class="n">arguments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Internal method to create an inbound node for the layer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input_tensors: list of input tensors.</span>
<span class="sd">        output_tensors: list of output tensors.</span>
<span class="sd">        arguments: dictionary of keyword arguments that were passed to the</span>
<span class="sd">            `call` method of the layer at the call that created the node.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inbound_layers</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">_keras_history</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span>
                                        <span class="n">input_tensors</span><span class="p">)</span>
    <span class="n">node_indices</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">_keras_history</span><span class="o">.</span><span class="n">node_index</span><span class="p">,</span>
                                      <span class="n">input_tensors</span><span class="p">)</span>
    <span class="n">tensor_indices</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">_keras_history</span><span class="o">.</span><span class="n">tensor_index</span><span class="p">,</span>
                                        <span class="n">input_tensors</span><span class="p">)</span>

    <span class="c1"># Create node, add it to inbound nodes.</span>
    <span class="n">Node</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inbound_layers</span><span class="o">=</span><span class="n">inbound_layers</span><span class="p">,</span>
        <span class="n">node_indices</span><span class="o">=</span><span class="n">node_indices</span><span class="p">,</span>
        <span class="n">tensor_indices</span><span class="o">=</span><span class="n">tensor_indices</span><span class="p">,</span>
        <span class="n">input_tensors</span><span class="o">=</span><span class="n">input_tensors</span><span class="p">,</span>
        <span class="n">output_tensors</span><span class="o">=</span><span class="n">output_tensors</span><span class="p">,</span>
        <span class="n">arguments</span><span class="o">=</span><span class="n">arguments</span><span class="p">)</span>

    <span class="c1"># Update tensor history metadata.</span>
    <span class="c1"># The metadata attribute consists of</span>
    <span class="c1"># 1) a layer instance</span>
    <span class="c1"># 2) a node index for the layer</span>
    <span class="c1"># 3) a tensor index for the node.</span>
    <span class="c1"># The allows layer reuse (multiple nodes per layer) and multi-output</span>
    <span class="c1"># or multi-input layers (e.g. a layer can return multiple tensors,</span>
    <span class="c1"># and each can be sent to a different layer).</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)):</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_keras_history</span> <span class="o">=</span> <span class="n">KerasHistory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                           <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">def</span> <span class="nf">_get_node_attribute_at_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Private utility to retrieves an attribute (e.g. inputs) from a node.</span>

<span class="sd">    This is used to implement the methods:</span>
<span class="sd">        - get_input_shape_at</span>
<span class="sd">        - get_output_shape_at</span>
<span class="sd">        - get_input_at</span>
<span class="sd">        etc...</span>

<span class="sd">    Arguments:</span>
<span class="sd">        node_index: Integer index of the node from which</span>
<span class="sd">            to retrieve the attribute.</span>
<span class="sd">        attr: Exact node attribute name.</span>
<span class="sd">        attr_name: Human-readable attribute name, for error messages.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The layer&#39;s attribute `attr` at the node of index `node_index`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If the layer has no inbound nodes, or if called in Eager</span>
<span class="sd">        mode.</span>
<span class="sd">        ValueError: If the index provided does not match any node.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The layer has never been called &#39;</span>
                         <span class="s1">&#39;and thus has no defined &#39;</span> <span class="o">+</span> <span class="n">attr_name</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">node_index</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Asked to get &#39;</span> <span class="o">+</span> <span class="n">attr_name</span> <span class="o">+</span> <span class="s1">&#39; at node &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, but the layer has only &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; inbound nodes.&#39;</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_index</span><span class="p">],</span> <span class="n">attr</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">values</span>

  <span class="k">def</span> <span class="nf">_maybe_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="c1"># Check input assumptions set before layer building, e.g. input rank.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">input_spec</span><span class="o">.</span><span class="n">assert_input_compatibility</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">input_list</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_list</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span><span class="o">.</span><span class="n">name</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="n">input_shapes</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_list</span><span class="p">):</span>
      <span class="n">input_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># Only call `build` if the user has manually overridden the build method.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">,</span> <span class="s1">&#39;_is_default&#39;</span><span class="p">):</span>
      <span class="c1"># Any setup work performed only once should happen in an `init_scope`</span>
      <span class="c1"># to avoid creating symbolic Tensors that will later pollute any eager</span>
      <span class="c1"># operations.</span>
      <span class="k">with</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">)</span>
    <span class="c1"># We must set self.built since user defined build functions are not</span>
    <span class="c1"># constrained to set self.built.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">_symbolic_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">input_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">output_shapes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_placeholder_like</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
      <span class="n">ph</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">ph</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">return</span> <span class="n">ph</span>

    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_make_placeholder_like</span><span class="p">,</span> <span class="n">output_shapes</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_obj_reference_counts</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A dictionary counting the number of attributes referencing an object.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_obj_reference_counts_dict&#39;</span><span class="p">,</span>
                                 <span class="n">object_identity</span><span class="o">.</span><span class="n">ObjectIdentityDictionary</span><span class="p">())</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_reference_counts_dict</span>

  <span class="k">def</span> <span class="nf">_maybe_create_attribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">default_value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create the attribute with the default value if it hasn&#39;t been created.</span>

<span class="sd">    This is useful for fields that is used for tracking purpose,</span>
<span class="sd">    _trainable_weights, or _layers. Note that user could create a layer subclass</span>
<span class="sd">    and assign an internal field before invoking the Layer.__init__(), the</span>
<span class="sd">    __setattr__() need to create the tracking fields and __init__() need to not</span>
<span class="sd">    override them.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: String, the name of the attribute.</span>
<span class="sd">      default_value: Object, the default value of the attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default_value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__delattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="c1"># For any super.__delattr__() call, we will directly use the implementation</span>
    <span class="c1"># in Trackable and skip the behavior in AutoTrackable. The Layer was</span>
    <span class="c1"># originally use Trackable as base class, the change of using Module as base</span>
    <span class="c1"># class forced us to have AutoTrackable in the class hierarchy. Skipping</span>
    <span class="c1"># the __delattr__ and __setattr__ in AutoTrackable will keep the status quo.</span>
    <span class="n">existing_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># If this value is replacing an existing object assigned to an attribute, we</span>
    <span class="c1"># should clean it out to avoid leaking memory. First we check if there are</span>
    <span class="c1"># other attributes referencing it.</span>
    <span class="n">reference_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_reference_counts</span>
    <span class="k">if</span> <span class="n">existing_value</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">reference_counts</span><span class="p">:</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="n">reference_count</span> <span class="o">=</span> <span class="n">reference_counts</span><span class="p">[</span><span class="n">existing_value</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">reference_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="c1"># There are other remaining references. We can&#39;t remove this object from</span>
      <span class="c1"># _layers etc.</span>
      <span class="n">reference_counts</span><span class="p">[</span><span class="n">existing_value</span><span class="p">]</span> <span class="o">=</span> <span class="n">reference_count</span> <span class="o">-</span> <span class="mi">1</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># This is the last remaining reference.</span>
      <span class="k">del</span> <span class="n">reference_counts</span><span class="p">[</span><span class="n">existing_value</span><span class="p">]</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_value</span><span class="p">,</span> <span class="n">Layer</span><span class="p">)</span>
        <span class="ow">or</span> <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">has_weights</span><span class="p">(</span><span class="n">existing_value</span><span class="p">)):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
          <span class="s1">&#39;_layers&#39;</span><span class="p">,</span>
          <span class="p">[</span><span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="k">if</span> <span class="n">l</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">existing_value</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_value</span><span class="p">,</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
          <span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span>
          <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">existing_value</span><span class="p">])</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span>
          <span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span>
          <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">existing_value</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;_self_setattr_tracking&#39;</span> <span class="ow">or</span>
        <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_self_setattr_tracking&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="ow">or</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span>
        <span class="c1"># Exclude @property.setters from tracking</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">name</span><span class="p">)):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="p">(</span><span class="s1">&#39;Can</span><span class="se">\&#39;</span><span class="s1">t set the attribute &quot;</span><span class="si">{}</span><span class="s1">&quot;, likely because it conflicts with &#39;</span>
             <span class="s1">&#39;an existing read-only @property of the object. Please choose a &#39;</span>
             <span class="s1">&#39;different name.&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
      <span class="k">return</span>

    <span class="c1"># Keep track of trackable objects, for the needs of `Network.save_weights`.</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">data_structures</span><span class="o">.</span><span class="n">sticky_attribute_assignment</span><span class="p">(</span>
        <span class="n">trackable</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="n">reference_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_obj_reference_counts</span>
    <span class="n">reference_counts</span><span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="n">reference_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># Clean out the old attribute, which clears _layers and _trainable_weights</span>
    <span class="c1"># if necessary.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="c1"># TODO(scottzhu): Need to track Module object as well for weight tracking.</span>
    <span class="c1"># Be careful about metric if it becomes a Module in future.</span>
    <span class="c1"># Append value to self._layers if relevant</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Layer</span><span class="p">)</span> <span class="ow">or</span>
        <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">has_weights</span><span class="p">(</span><span class="n">value</span><span class="p">)):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="p">[])</span>
      <span class="c1"># We need to check object identity to avoid de-duplicating empty</span>
      <span class="c1"># container types which compare equal.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">((</span><span class="n">layer</span> <span class="ow">is</span> <span class="n">value</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;_use_resource_variables&#39;</span><span class="p">):</span>
          <span class="c1"># Legacy layers (V1 tf.layers) must always use</span>
          <span class="c1"># resource variables.</span>
          <span class="n">value</span><span class="o">.</span><span class="n">_use_resource_variables</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Append value to list of trainable / non-trainable weights if relevant</span>
    <span class="c1"># TODO(b/125122625): This won&#39;t pick up on any variables added to a</span>
    <span class="c1"># list/dict after creation.</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
      <span class="c1"># TODO(b/126450014): Remove `_UnreadVariable` check here when assign ops</span>
      <span class="c1"># no longer return True for isinstance Variable checks.</span>
      <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">tf_variables</span><span class="o">.</span><span class="n">Variable</span><span class="p">)</span> <span class="ow">and</span>
          <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">_UnreadVariable</span><span class="p">)):</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="c1"># Users may add extra weights/variables</span>
        <span class="c1"># simply by assigning them to attributes (invalid for graph networks)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="k">if</span> <span class="n">val</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
          <span class="n">backend</span><span class="o">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="c1"># Skip the auto trackable from tf.Module to keep status quo. See the comment</span>
    <span class="c1"># at __delattr__.</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">tracking</span><span class="o">.</span><span class="n">AutoTrackable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_gather_children_attribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attribute</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="p">{</span>
        <span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="s1">&#39;trainable_weights&#39;</span><span class="p">,</span> <span class="s1">&#39;non_trainable_weights&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">,</span>
        <span class="s1">&#39;losses&#39;</span><span class="p">,</span> <span class="s1">&#39;metrics&#39;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_layers&#39;</span><span class="p">):</span>
      <span class="n">nested_layers</span> <span class="o">=</span> <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">filter_empty_layer_containers</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
      <span class="k">return</span> <span class="nb">list</span><span class="p">(</span>
          <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span>
              <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">attribute</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">nested_layers</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[]</span>

  <span class="c1"># This is a hack so that the is_layer (within</span>
  <span class="c1"># training/trackable/layer_utils.py) check doesn&#39;t get the weights attr.</span>
  <span class="c1"># TODO(b/110718070): Remove when fixed.</span>
  <span class="k">def</span> <span class="nf">_is_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">True</span>

  <span class="nd">@property</span>
  <span class="nd">@tracking</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_call_fn_args</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">function_utils</span><span class="o">.</span><span class="n">fn_args</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="nd">@tracking</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_should_compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="s1">&#39;mask&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span> <span class="ow">or</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;compute_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Node</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A `Node` describes the connectivity between two layers.</span>

<span class="sd">  Each time a layer is connected to some new input,</span>
<span class="sd">  a node is added to `layer._inbound_nodes`.</span>
<span class="sd">  Each time the output of a layer is used by another layer,</span>
<span class="sd">  a node is added to `layer._outbound_nodes`.</span>

<span class="sd">  Arguments:</span>
<span class="sd">      outbound_layer: the layer that takes</span>
<span class="sd">          `input_tensors` and turns them into `output_tensors`</span>
<span class="sd">          (the node gets created when the `call`</span>
<span class="sd">          method of the layer was called).</span>
<span class="sd">      inbound_layers: a list of layers, the same length as `input_tensors`,</span>
<span class="sd">          the layers from where `input_tensors` originate.</span>
<span class="sd">      node_indices: a list of integers, the same length as `inbound_layers`.</span>
<span class="sd">          `node_indices[i]` is the origin node of `input_tensors[i]`</span>
<span class="sd">          (necessary since each inbound layer might have several nodes,</span>
<span class="sd">          e.g. if the layer is being shared with a different data stream).</span>
<span class="sd">      tensor_indices: a list of integers,</span>
<span class="sd">          the same length as `inbound_layers`.</span>
<span class="sd">          `tensor_indices[i]` is the index of `input_tensors[i]` within the</span>
<span class="sd">          output of the inbound layer</span>
<span class="sd">          (necessary since each inbound layer might</span>
<span class="sd">          have multiple tensor outputs, with each one being</span>
<span class="sd">          independently manipulable).</span>
<span class="sd">      input_tensors: list of input tensors.</span>
<span class="sd">      output_tensors: list of output tensors.</span>
<span class="sd">      arguments: dictionary of keyword arguments that were passed to the</span>
<span class="sd">          `call` method of the layer at the call that created the node.</span>

<span class="sd">  `node_indices` and `tensor_indices` are basically fine-grained coordinates</span>
<span class="sd">  describing the origin of the `input_tensors`.</span>

<span class="sd">  A node from layer A to layer B is added to:</span>
<span class="sd">    - A._outbound_nodes</span>
<span class="sd">    - B._inbound_nodes</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">outbound_layer</span><span class="p">,</span>
               <span class="n">inbound_layers</span><span class="p">,</span>
               <span class="n">node_indices</span><span class="p">,</span>
               <span class="n">tensor_indices</span><span class="p">,</span>
               <span class="n">input_tensors</span><span class="p">,</span>
               <span class="n">output_tensors</span><span class="p">,</span>
               <span class="n">arguments</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Layer instance (NOT a sequence)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outbound_layer</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`outbound_layer` should be a layer instance, &#39;</span>
                       <span class="s1">&#39;not a list, tuple, or, dict.&#39;</span><span class="p">)</span>

    <span class="c1"># this is the layer that takes a nested structure of input tensors</span>
    <span class="c1"># and turns them into a nested structure of output tensors.</span>
    <span class="c1"># the current node will be added to</span>
    <span class="c1"># the inbound_nodes of outbound_layer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">outbound_layer</span> <span class="o">=</span> <span class="n">outbound_layer</span>

    <span class="c1"># The following 3 properties describe where</span>
    <span class="c1"># the input tensors come from: which layers,</span>
    <span class="c1"># and for each layer, which node and which</span>
    <span class="c1"># tensor output of each node.</span>

    <span class="c1"># Nested structure of layer instances.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inbound_layers</span> <span class="o">=</span> <span class="n">inbound_layers</span>
    <span class="c1"># Nested structure of integers, 1:1 mapping with inbound_layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">node_indices</span> <span class="o">=</span> <span class="n">node_indices</span>
    <span class="c1"># Nested of integers, 1:1 mapping with inbound_layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tensor_indices</span> <span class="o">=</span> <span class="n">tensor_indices</span>

    <span class="c1"># Following 2 properties:</span>
    <span class="c1"># tensor inputs and outputs of outbound_layer.</span>

    <span class="c1"># Nested structure of tensors. 1:1 mapping with inbound_layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_tensors</span> <span class="o">=</span> <span class="n">input_tensors</span>
    <span class="c1"># Nested structure of tensors, created by outbound_layer.call().</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_tensors</span> <span class="o">=</span> <span class="n">output_tensors</span>

    <span class="c1"># Following 2 properties: input and output shapes.</span>

    <span class="c1"># Nested structure of shape tuples, shapes of input_tensors.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">backend</span><span class="o">.</span><span class="n">int_shape</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">)</span>
    <span class="c1"># Nested structure of shape tuples, shapes of output_tensors.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">backend</span><span class="o">.</span><span class="n">int_shape</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">)</span>

    <span class="c1"># Optional keyword arguments to layer&#39;s `call`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">arguments</span> <span class="o">=</span> <span class="n">arguments</span>

    <span class="c1"># Add nodes to all layers involved.</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inbound_layers</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># For compatibility with external Keras, we use the deprecated</span>
        <span class="c1"># accessor here.</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">outbound_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="c1"># For compatibility with external Keras, we use the deprecated</span>
    <span class="c1"># accessor here.</span>
    <span class="n">outbound_layer</span><span class="o">.</span><span class="n">inbound_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">iterate_inbound</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a list of tuples representing the inbound data.</span>

<span class="sd">    Returns:</span>
<span class="sd">      List of tuples like: (inbound_layer, node_index, tensor_index, tensor).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inbound_layers</span><span class="p">),</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_indices</span><span class="p">),</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_indices</span><span class="p">),</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_tensors</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">inbound_names</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">layer</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="k">if</span> <span class="n">layer</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inbound_layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;outbound_layer&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outbound_layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="s1">&#39;inbound_layers&#39;</span><span class="p">:</span> <span class="n">inbound_names</span><span class="p">,</span>
        <span class="s1">&#39;node_indices&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_indices</span><span class="p">,</span>
        <span class="s1">&#39;tensor_indices&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_indices</span>
    <span class="p">}</span>


<span class="k">class</span> <span class="nc">TensorFlowOpLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Wraps a TensorFlow Operation in a Layer.</span>

<span class="sd">  This class is used internally by the Functional API. When a user</span>
<span class="sd">  uses a raw TensorFlow Operation on symbolic tensors originating</span>
<span class="sd">  from an `Input` Layer, the resultant operation will be wrapped</span>
<span class="sd">  with this Layer object in order to make the operation compatible</span>
<span class="sd">  with the Keras API.</span>

<span class="sd">  This Layer will create a new, identical operation (except for inputs</span>
<span class="sd">  and outputs) every time it is called. If `run_eagerly` is `True`,</span>
<span class="sd">  the op creation and calculation will happen inside an Eager function.</span>

<span class="sd">  Instances of this Layer are created when `autolambda` is called, which</span>
<span class="sd">  is whenever a Layer&#39;s `__call__` encounters symbolic inputs that do</span>
<span class="sd">  not have Keras metadata, or when a Network&#39;s `__init__` encounters</span>
<span class="sd">  outputs that do not have Keras metadata.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    node_def: String, the serialized NodeDef of the Op this layer will wrap.</span>
<span class="sd">    constants: Dict of NumPy arrays, the values of any Tensors needed for this</span>
<span class="sd">      Operation that do not originate from a Keras `Input` Layer. Since all</span>
<span class="sd">      placeholders must come from Keras `Input` Layers, these Tensors must be</span>
<span class="sd">      treated as constant in the Functional API.</span>
<span class="sd">    name: String, the name of the Layer.</span>
<span class="sd">    trainable: Bool, whether this Layer is trainable. Currently Variables are</span>
<span class="sd">      not supported, and so this parameter has no effect.</span>
<span class="sd">    dtype: The default dtype of this Layer. Inherited from `Layer` and has no</span>
<span class="sd">      effect on this class, however is used in `get_config`.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">node_def</span><span class="p">,</span>
               <span class="n">constants</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TensorFlowOpLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">_TF_OP_LAYER_NAME_PREFIX</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span> <span class="o">=</span> <span class="n">node_def_pb2</span><span class="o">.</span><span class="n">NodeDef</span><span class="o">.</span><span class="n">FromString</span><span class="p">(</span><span class="n">node_def</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">constants</span> <span class="o">=</span> <span class="n">constants</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="c1"># Layer uses original op unless it is called on new inputs.</span>
    <span class="c1"># This means `built` is not set in `__call__`.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_defun_call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_op</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_make_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">graph</span>
    <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">constant</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">constant</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">constant</span><span class="p">)</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">constant</span><span class="p">)</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">unique_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="c1"># Check for case where first input should be a list of Tensors.</span>
      <span class="k">if</span> <span class="s1">&#39;N&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">:</span>
        <span class="n">num_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">attr</span><span class="p">[</span><span class="s1">&#39;N&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">i</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">[:</span><span class="n">num_tensors</span><span class="p">]]</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="n">num_tensors</span><span class="p">:]</span>
      <span class="n">c_op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">_create_c_op</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">control_inputs</span><span class="o">=</span><span class="p">[])</span>
      <span class="n">op</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">_create_op_from_tf_operation</span><span class="p">(</span><span class="n">c_op</span><span class="p">)</span>

      <span class="c1"># Record the gradient because custom-made ops don&#39;t go through the</span>
      <span class="c1"># code-gen&#39;d eager call path</span>
      <span class="n">op_type</span> <span class="o">=</span> <span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">op_def</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="n">attr_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">attr</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">op</span><span class="o">.</span><span class="n">op_def</span><span class="o">.</span><span class="n">attr</span><span class="p">]</span>
      <span class="n">attrs</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">attr_names</span><span class="p">:</span>
        <span class="n">attrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attr_name</span><span class="p">)</span>
        <span class="n">attrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="n">attr_name</span><span class="p">))</span>
      <span class="n">attrs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">attrs</span><span class="p">)</span>
      <span class="n">execute</span><span class="o">.</span><span class="n">record_gradient</span><span class="p">(</span><span class="n">op_type</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span>
                              <span class="n">op</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">return</span> <span class="n">op</span><span class="o">.</span><span class="n">outputs</span>

  <span class="nd">@function</span><span class="o">.</span><span class="n">defun</span>
  <span class="k">def</span> <span class="nf">_defun_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps the op creation method in an Eager function for `run_eagerly`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_op</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">TensorFlowOpLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;node_def&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">(),</span>
        <span class="s1">&#39;constants&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="n">i</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">config</span>


<span class="k">class</span> <span class="nc">AddLoss</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds its inputs as a loss.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    unconditional: Whether or not the loss should be conditioned on the inputs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unconditional</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AddLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unconditional</span> <span class="o">=</span> <span class="n">unconditional</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">inputs</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AddLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;unconditional&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">unconditional</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">config</span>


<span class="k">class</span> <span class="nc">AddMetric</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Adds its inputs as a metric.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    aggregation: &#39;mean&#39; or None. How the inputs should be aggregated.</span>
<span class="sd">    metric_name: The name to use for this metric.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AddMetric</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">metric_name</span> <span class="o">=</span> <span class="n">metric_name</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">AddMetric</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="s1">&#39;aggregation&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
        <span class="s1">&#39;metric_name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_name</span>
    <span class="p">})</span>
    <span class="k">return</span> <span class="n">config</span>


<span class="k">class</span> <span class="nc">KerasHistory</span><span class="p">(</span>
    <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;KerasHistory&#39;</span><span class="p">,</span>
                           <span class="p">[</span><span class="s1">&#39;layer&#39;</span><span class="p">,</span> <span class="s1">&#39;node_index&#39;</span><span class="p">,</span> <span class="s1">&#39;tensor_index&#39;</span><span class="p">])):</span>
  <span class="sd">&quot;&quot;&quot;Tracks the Layer call that created a Tensor, for Keras Graph Networks.</span>

<span class="sd">  During construction of Keras Graph Networks, this metadata is added to</span>
<span class="sd">  each Tensor produced as the output of a Layer, starting with an</span>
<span class="sd">  `InputLayer`. This allows Keras to track how each Tensor was produced, and</span>
<span class="sd">  this information is later retraced by the `keras.engine.Network` class to</span>
<span class="sd">  reconstruct the Keras Graph Network.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    layer: The Layer that produced the Tensor.</span>
<span class="sd">    node_index: The specific call to the Layer that produced this Tensor. Layers</span>
<span class="sd">      can be called multiple times in order to share weights. A new node is</span>
<span class="sd">      created every time a Tensor is called.</span>
<span class="sd">    tensor_index: The output index for this Tensor. Always zero if the Layer</span>
<span class="sd">      that produced this Tensor only has one output. Nested structures of</span>
<span class="sd">      Tensors are deterministically assigned an index via `nest.flatten`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Added to maintain memory and performance characteristics of `namedtuple`</span>
  <span class="c1"># while subclassing.</span>
  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">()</span>


<span class="k">def</span> <span class="nf">default</span><span class="p">(</span><span class="n">method</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Decorates a method to detect overrides in subclasses.&quot;&quot;&quot;</span>
  <span class="n">method</span><span class="o">.</span><span class="n">_is_default</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">return</span> <span class="n">method</span>


<span class="c1"># Avoid breaking users who directly import this symbol from this file.</span>
<span class="c1"># TODO(fchollet): remove this.</span>
<span class="n">InputSpec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span>  <span class="c1"># pylint:disable=invalid-name</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>