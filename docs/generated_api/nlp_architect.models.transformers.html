

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nlp_architect.models.transformers package &mdash; NLP Architect by IntelÂ® AI Lab 0.5.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/install.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nlp_arch_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:100,900" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="nlp_architect.nlp package" href="nlp_architect.nlp.html" />
    <link rel="prev" title="nlp_architect.models.gnmt.utils package" href="nlp_architect.models.gnmt.utils.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start.html">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Jupyter Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo.html">Model Zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">NLP/NLU Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tagging/sequence_tagging.html">Sequence Tagging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sentiment.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bist_parser.html">Dependency Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intent.html">Intent Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm.html">Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../information_extraction.html">Information Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../archived/additional.html">Additional Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Optimized Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantized_bert.html">Quantized BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers_distillation.html">Transformers Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_gnmt.html">Sparse Neural Machine Translation</a></li>
</ul>
<p class="caption"><span class="caption-text">Solutions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../absa_solution.html">Aspect Based Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../term_set_expansion.html">Set Expansion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trend_analysis.html">Trend Analysis</a></li>
</ul>
<p class="caption"><span class="caption-text">For Developers</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="nlp_architect_api_index.html">nlp_architect API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.api.html">nlp_architect.api package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.cli.html">nlp_architect.cli package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.common.html">nlp_architect.common package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.data.html">nlp_architect.data package</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="nlp_architect.models.html">nlp_architect.models package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="nlp_architect.models.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="nlp_architect.models.absa.html">nlp_architect.models.absa package</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp_architect.models.bist.html">nlp_architect.models.bist package</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp_architect.models.cross_doc_coref.html">nlp_architect.models.cross_doc_coref package</a></li>
<li class="toctree-l4"><a class="reference internal" href="nlp_architect.models.gnmt.html">nlp_architect.models.gnmt package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">nlp_architect.models.transformers package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.bist_parser">nlp_architect.models.bist_parser module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.chunker">nlp_architect.models.chunker module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.cross_doc_sieves">nlp_architect.models.cross_doc_sieves module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.crossling_emb">nlp_architect.models.crossling_emb module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.gnmt_model">nlp_architect.models.gnmt_model module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.intent_extraction">nlp_architect.models.intent_extraction module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.matchlstm_ansptr">nlp_architect.models.matchlstm_ansptr module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.memn2n_dialogue">nlp_architect.models.memn2n_dialogue module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.most_common_word_sense">nlp_architect.models.most_common_word_sense module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.ner_crf">nlp_architect.models.ner_crf module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.np2vec">nlp_architect.models.np2vec module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.np_semantic_segmentation">nlp_architect.models.np_semantic_segmentation module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.pretrained_models">nlp_architect.models.pretrained_models module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.supervised_sentiment">nlp_architect.models.supervised_sentiment module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.tagging">nlp_architect.models.tagging module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models.temporal_convolutional_network">nlp_architect.models.temporal_convolutional_network module</a></li>
<li class="toctree-l3"><a class="reference internal" href="nlp_architect.models.html#module-nlp_architect.models">Module contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.nlp.html">nlp_architect.nlp package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.nn.html">nlp_architect.nn package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.pipelines.html">nlp_architect.pipelines package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.procedures.html">nlp_architect.procedures package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.solutions.html">nlp_architect.solutions package</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp_architect.utils.html">nlp_architect.utils package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide.html">Developer Guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NLP Architect by IntelÂ® AI Lab</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="nlp_architect_api_index.html"><code class="docutils literal notranslate"><span class="pre">nlp\_architect</span></code> package</a> &raquo;</li>
        
          <li><a href="nlp_architect.models.html">nlp_architect.models package</a> &raquo;</li>
        
      <li>nlp_architect.models.transformers package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nlp-architect-models-transformers-package">
<h1>nlp_architect.models.transformers package<a class="headerlink" href="#nlp-architect-models-transformers-package" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">Â¶</a></h2>
</div>
<div class="section" id="module-nlp_architect.models.transformers.base_model">
<span id="nlp-architect-models-transformers-base-model-module"></span><h2>nlp_architect.models.transformers.base_model module<a class="headerlink" href="#module-nlp_architect.models.transformers.base_model" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="nlp_architect.models.transformers.base_model.InputFeatures">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.base_model.</code><code class="descname">InputFeatures</code><span class="sig-paren">(</span><em>input_ids</em>, <em>input_mask</em>, <em>segment_ids</em>, <em>label_id=None</em>, <em>valid_ids=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#InputFeatures"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.InputFeatures" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A single set of features of data.</p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.base_model.</code><code class="descname">TransformerBase</code><span class="sig-paren">(</span><em>model_type: str</em>, <em>model_name_or_path: str</em>, <em>labels: List[str] = None</em>, <em>num_labels: int = None</em>, <em>config_name=None</em>, <em>tokenizer_name=None</em>, <em>do_lower_case=False</em>, <em>output_path=None</em>, <em>device='cpu'</em>, <em>n_gpus=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nlp_architect.models.html#nlp_architect.models.TrainableModel" title="nlp_architect.models.TrainableModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.TrainableModel</span></code></a></p>
<p>Transformers base model (for working with pytorch-transformers models)</p>
<dl class="attribute">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.MODEL_CONFIGURATIONS">
<code class="descname">MODEL_CONFIGURATIONS</code><em class="property"> = {'bert': (&lt;class 'transformers.configuration_bert.BertConfig'&gt;, &lt;class 'transformers.tokenization_bert.BertTokenizer'&gt;), 'quant_bert': (&lt;class 'nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig'&gt;, &lt;class 'transformers.tokenization_bert.BertTokenizer'&gt;), 'roberta': (&lt;class 'transformers.configuration_roberta.RobertaConfig'&gt;, &lt;class 'transformers.tokenization_roberta.RobertaTokenizer'&gt;), 'xlm': (&lt;class 'transformers.configuration_xlm.XLMConfig'&gt;, &lt;class 'transformers.tokenization_xlm.XLMTokenizer'&gt;), 'xlnet': (&lt;class 'transformers.configuration_xlnet.XLNetConfig'&gt;, &lt;class 'transformers.tokenization_xlnet.XLNetTokenizer'&gt;)}</em><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.MODEL_CONFIGURATIONS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.evaluate_predictions">
<code class="descname">evaluate_predictions</code><span class="sig-paren">(</span><em>logits</em>, <em>label_ids</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.evaluate_predictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.evaluate_predictions" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.get_logits">
<code class="descname">get_logits</code><span class="sig-paren">(</span><em>batch</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.get_logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.get_logits" title="Permalink to this definition">Â¶</a></dt>
<dd><p>get model logits from given input</p>
</dd></dl>

<dl class="staticmethod">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.get_train_steps_epochs">
<em class="property">static </em><code class="descname">get_train_steps_epochs</code><span class="sig-paren">(</span><em>max_steps: int</em>, <em>num_train_epochs: int</em>, <em>gradient_accumulation_steps: int</em>, <em>num_samples: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.get_train_steps_epochs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.get_train_steps_epochs" title="Permalink to this definition">Â¶</a></dt>
<dd><p>get train steps and epochs</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>max_steps</strong> (<em>int</em>) â max steps</li>
<li><strong>num_train_epochs</strong> (<em>int</em>) â num epochs</li>
<li><strong>gradient_accumulation_steps</strong> (<em>int</em>) â gradient accumulation steps</li>
<li><strong>num_samples</strong> (<em>int</em>) â number of samples</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">total steps, number of epochs</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Tuple</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.load_model">
<em class="property">classmethod </em><code class="descname">load_model</code><span class="sig-paren">(</span><em>model_path: str</em>, <em>model_type: str</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.load_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.load_model" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Create a TranformerBase deom from given path</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>model_path</strong> (<em>str</em>) â path to model</li>
<li><strong>model_type</strong> (<em>str</em>) â model type</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">model</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#nlp_architect.models.transformers.base_model.TransformerBase" title="nlp_architect.models.transformers.base_model.TransformerBase">TransformerBase</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.optimizer">
<code class="descname">optimizer</code><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.optimizer" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.save_model">
<code class="descname">save_model</code><span class="sig-paren">(</span><em>output_dir: str</em>, <em>save_checkpoint: bool = False</em>, <em>args=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.save_model" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Save model/tokenizer/arguments to given output directory</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_dir</strong> (<em>str</em>) â path to output directory</li>
<li><strong>save_checkpoint</strong> (<em>bool</em><em>, </em><em>optional</em>) â save as checkpoint. Defaults to False.</li>
<li><strong>args</strong> (<em>[</em><em>type</em><em>]</em><em>, </em><em>optional</em>) â arguments object to save. Defaults to None.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.save_model_checkpoint">
<code class="descname">save_model_checkpoint</code><span class="sig-paren">(</span><em>output_path: str</em>, <em>name: str</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.save_model_checkpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.save_model_checkpoint" title="Permalink to this definition">Â¶</a></dt>
<dd><p>save model checkpoint</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_path</strong> (<em>str</em>) â output path</li>
<li><strong>name</strong> (<em>str</em>) â name of checkpoint</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.scheduler">
<code class="descname">scheduler</code><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.scheduler" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.setup_default_optimizer">
<code class="descname">setup_default_optimizer</code><span class="sig-paren">(</span><em>weight_decay: float = 0.0</em>, <em>learning_rate: float = 5e-05</em>, <em>adam_epsilon: float = 1e-08</em>, <em>warmup_steps: int = 0</em>, <em>total_steps: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.setup_default_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.setup_default_optimizer" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.base_model.TransformerBase.to">
<code class="descname">to</code><span class="sig-paren">(</span><em>device='cpu'</em>, <em>n_gpus=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#TransformerBase.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.TransformerBase.to" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="nlp_architect.models.transformers.base_model.get_models">
<code class="descclassname">nlp_architect.models.transformers.base_model.</code><code class="descname">get_models</code><span class="sig-paren">(</span><em>models: List[str]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/base_model.html#get_models"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.base_model.get_models" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nlp_architect.models.transformers.quantized_bert">
<span id="nlp-architect-models-transformers-quantized-bert-module"></span><h2>nlp_architect.models.transformers.quantized_bert module<a class="headerlink" href="#module-nlp_architect.models.transformers.quantized_bert" title="Permalink to this headline">Â¶</a></h2>
<p>Quantized BERT layers and model</p>
<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertAttention">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertAttention</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertAttention" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertAttention</span></code></p>
<dl class="method">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertAttention.prune_heads">
<code class="descname">prune_heads</code><span class="sig-paren">(</span><em>heads</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertAttention.prune_heads"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertAttention.prune_heads" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertConfig</code><span class="sig-paren">(</span><em>vocab_size_or_config_json_file=30522</em>, <em>hidden_size=768</em>, <em>num_hidden_layers=12</em>, <em>num_attention_heads=12</em>, <em>intermediate_size=3072</em>, <em>hidden_act='gelu'</em>, <em>hidden_dropout_prob=0.1</em>, <em>attention_probs_dropout_prob=0.1</em>, <em>max_position_embeddings=512</em>, <em>type_vocab_size=2</em>, <em>initializer_range=0.02</em>, <em>layer_norm_eps=1e-12</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_bert.BertConfig</span></code></p>
<dl class="attribute">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig.pretrained_config_archive_map">
<code class="descname">pretrained_config_archive_map</code><em class="property"> = {'bert-base-uncased': 'https://nlp-architect-data.s3-us-west-2.amazonaws.com/models/transformers/bert-base-uncased.json', 'bert-large-uncased': 'https://nlp-architect-data.s3-us-west-2.amazonaws.com/models/transformers/bert-large-uncased.json'}</em><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig.pretrained_config_archive_map" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertEmbeddings">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertEmbeddings</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertEmbeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertEmbeddings" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertEmbeddings</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertEncoder">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertEncoder</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertEncoder" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertEncoder</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertForQuestionAnswering">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertForQuestionAnswering</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertForQuestionAnswering"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertForQuestionAnswering" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel" title="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertForQuestionAnswering</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertForSequenceClassification">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertForSequenceClassification</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertForSequenceClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertForSequenceClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel" title="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertForSequenceClassification</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertForTokenClassification">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertForTokenClassification</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertForTokenClassification"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertForTokenClassification" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel" title="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertForTokenClassification</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertIntermediate">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertIntermediate</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertIntermediate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertIntermediate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertIntermediate</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertLayer">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertLayer</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertLayer" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertLayer</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertModel">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertModel</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel" title="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertModel</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertOutput">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertOutput</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertOutput" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertOutput</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPooler">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertPooler</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertPooler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPooler" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertPooler</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertPreTrainedModel</code><span class="sig-paren">(</span><em>config</em>, <em>*inputs</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertPreTrainedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertPreTrainedModel</span></code></p>
<dl class="attribute">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.base_model_prefix">
<code class="descname">base_model_prefix</code><em class="property"> = 'quant_bert'</em><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.base_model_prefix" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.config_class">
<code class="descname">config_class</code><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.config_class" title="Permalink to this definition">Â¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig" title="nlp_architect.models.transformers.quantized_bert.QuantizedBertConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedBertConfig</span></code></a></p>
</dd></dl>

<dl class="classmethod">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>pretrained_model_name_or_path</em>, <em>*args</em>, <em>from_8bit=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertPreTrainedModel.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.from_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>load trained model from 8bit model</p>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.init_weights">
<code class="descname">init_weights</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertPreTrainedModel.init_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.init_weights" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Initialize the weights.</p>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.save_pretrained">
<code class="descname">save_pretrained</code><span class="sig-paren">(</span><em>save_directory</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertPreTrainedModel.save_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.save_pretrained" title="Permalink to this definition">Â¶</a></dt>
<dd><p>save trained model in 8bit</p>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.toggle_8bit">
<code class="descname">toggle_8bit</code><span class="sig-paren">(</span><em>mode: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertPreTrainedModel.toggle_8bit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertPreTrainedModel.toggle_8bit" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertSelfAttention">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertSelfAttention</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertSelfAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertSelfAttention" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertSelfAttention</span></code></p>
</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.quantized_bert.QuantizedBertSelfOutput">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">QuantizedBertSelfOutput</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#QuantizedBertSelfOutput"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertSelfOutput" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertSelfOutput</span></code></p>
</dd></dl>

<dl class="function">
<dt id="nlp_architect.models.transformers.quantized_bert.quantized_embedding_setup">
<code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">quantized_embedding_setup</code><span class="sig-paren">(</span><em>config</em>, <em>name</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#quantized_embedding_setup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.quantized_embedding_setup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get QuantizedEmbedding layer according to config params</p>
</dd></dl>

<dl class="function">
<dt id="nlp_architect.models.transformers.quantized_bert.quantized_linear_setup">
<code class="descclassname">nlp_architect.models.transformers.quantized_bert.</code><code class="descname">quantized_linear_setup</code><span class="sig-paren">(</span><em>config</em>, <em>name</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/quantized_bert.html#quantized_linear_setup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.quantized_bert.quantized_linear_setup" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Get QuantizedLinear layer according to config params</p>
</dd></dl>

</div>
<div class="section" id="module-nlp_architect.models.transformers.sequence_classification">
<span id="nlp-architect-models-transformers-sequence-classification-module"></span><h2>nlp_architect.models.transformers.sequence_classification module<a class="headerlink" href="#module-nlp_architect.models.transformers.sequence_classification" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.sequence_classification.</code><code class="descname">TransformerSequenceClassifier</code><span class="sig-paren">(</span><em>model_type: str</em>, <em>labels: List[str] = None</em>, <em>task_type='classification'</em>, <em>metric_fn=&lt;function accuracy&gt;</em>, <em>load_quantized=False</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/sequence_classification.html#TransformerSequenceClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.base_model.TransformerBase" title="nlp_architect.models.transformers.base_model.TransformerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.base_model.TransformerBase</span></code></a></p>
<p>Transformer sequence classifier</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>model_type</strong> (<em>str</em>) â transformer base model type</li>
<li><strong>labels</strong> (<em>List</em><em>[</em><em>str</em><em>]</em><em>, </em><em>optional</em>) â list of labels. Defaults to None.</li>
<li><strong>task_type</strong> (<em>str</em><em>, </em><em>optional</em>) â task type (classification/regression). Defaults to</li>
<li><strong>classification.</strong> â </li>
<li><strong>metric_fn</strong> (<em>[</em><em>type</em><em>]</em><em>, </em><em>optional</em>) â metric to use for evaluation. Defaults to</li>
<li><strong>simple_accuracy.</strong> â </li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.MODEL_CLASS">
<code class="descname">MODEL_CLASS</code><em class="property"> = {'bert': &lt;class 'transformers.modeling_bert.BertForSequenceClassification'&gt;, 'quant_bert': &lt;class 'nlp_architect.models.transformers.quantized_bert.QuantizedBertForSequenceClassification'&gt;, 'roberta': &lt;class 'transformers.modeling_roberta.RobertaForSequenceClassification'&gt;, 'xlm': &lt;class 'transformers.modeling_xlm.XLMForSequenceClassification'&gt;, 'xlnet': &lt;class 'transformers.modeling_xlnet.XLNetForSequenceClassification'&gt;}</em><a class="headerlink" href="#nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.MODEL_CLASS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.convert_to_tensors">
<code class="descname">convert_to_tensors</code><span class="sig-paren">(</span><em>examples: List[nlp_architect.data.sequence_classification.SequenceClsInputExample], max_seq_length: int = 128, include_labels: bool = True</em><span class="sig-paren">)</span> &#x2192; torch.utils.data.dataset.TensorDataset<a class="reference internal" href="../_modules/nlp_architect/models/transformers/sequence_classification.html#TransformerSequenceClassifier.convert_to_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.convert_to_tensors" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Convert examples to tensor dataset</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>examples</strong> (<em>List</em><em>[</em><a class="reference internal" href="nlp_architect.data.html#nlp_architect.data.sequence_classification.SequenceClsInputExample" title="nlp_architect.data.sequence_classification.SequenceClsInputExample"><em>SequenceClsInputExample</em></a><em>]</em>) â examples</li>
<li><strong>max_seq_length</strong> (<em>int</em><em>, </em><em>optional</em>) â max sequence length. Defaults to 128.</li>
<li><strong>include_labels</strong> (<em>bool</em><em>, </em><em>optional</em>) â include labels. Defaults to True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">TensorDataset</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.evaluate_predictions">
<code class="descname">evaluate_predictions</code><span class="sig-paren">(</span><em>logits</em>, <em>label_ids</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/sequence_classification.html#TransformerSequenceClassifier.evaluate_predictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.evaluate_predictions" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Run evaluation of given logits and truth labels</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>logits</strong> â model logits</li>
<li><strong>label_ids</strong> â truth label ids</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><em>examples: List[nlp_architect.data.sequence_classification.SequenceClsInputExample], max_seq_length: int, batch_size: int = 64, evaluate=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/sequence_classification.html#TransformerSequenceClassifier.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.inference" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Run inference on given examples</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>examples</strong> (<em>List</em><em>[</em><a class="reference internal" href="nlp_architect.data.html#nlp_architect.data.sequence_classification.SequenceClsInputExample" title="nlp_architect.data.sequence_classification.SequenceClsInputExample"><em>SequenceClsInputExample</em></a><em>]</em>) â examples</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) â batch size. Defaults to 64.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">logits</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>train_data_set: torch.utils.data.dataloader.DataLoader</em>, <em>dev_data_set: Union[torch.utils.data.dataloader.DataLoader</em>, <em>List[torch.utils.data.dataloader.DataLoader]] = None</em>, <em>test_data_set: Union[torch.utils.data.dataloader.DataLoader</em>, <em>List[torch.utils.data.dataloader.DataLoader]] = None</em>, <em>gradient_accumulation_steps: int = 1</em>, <em>per_gpu_train_batch_size: int = 8</em>, <em>max_steps: int = -1</em>, <em>num_train_epochs: int = 3</em>, <em>max_grad_norm: float = 1.0</em>, <em>logging_steps: int = 50</em>, <em>save_steps: int = 100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/sequence_classification.html#TransformerSequenceClassifier.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.sequence_classification.TransformerSequenceClassifier.train" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Train a model</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>train_data_set</strong> (<em>DataLoader</em>) â training data set</li>
<li><strong>dev_data_set</strong> (<em>Union</em><em>[</em><em>DataLoader</em><em>, </em><em>List</em><em>[</em><em>DataLoader</em><em>]</em><em>]</em><em>, </em><em>optional</em>) â development set.</li>
<li><strong>to None.</strong> (<em>Defaults</em>) â </li>
<li><strong>test_data_set</strong> (<em>Union</em><em>[</em><em>DataLoader</em><em>, </em><em>List</em><em>[</em><em>DataLoader</em><em>]</em><em>]</em><em>, </em><em>optional</em>) â test set.</li>
<li><strong>to None.</strong> â </li>
<li><strong>gradient_accumulation_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â num of gradient accumulation steps.</li>
<li><strong>to 1.</strong> (<em>Defaults</em>) â </li>
<li><strong>per_gpu_train_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) â per GPU train batch size. Defaults to 8.</li>
<li><strong>max_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â max steps. Defaults to -1.</li>
<li><strong>num_train_epochs</strong> (<em>int</em><em>, </em><em>optional</em>) â number of train epochs. Defaults to 3.</li>
<li><strong>max_grad_norm</strong> (<em>float</em><em>, </em><em>optional</em>) â max gradient normalization. Defaults to 1.0.</li>
<li><strong>logging_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â number of steps between logging. Defaults to 50.</li>
<li><strong>save_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â number of steps between model save. Defaults to 100.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nlp_architect.models.transformers.token_classification">
<span id="nlp-architect-models-transformers-token-classification-module"></span><h2>nlp_architect.models.transformers.token_classification module<a class="headerlink" href="#module-nlp_architect.models.transformers.token_classification" title="Permalink to this headline">Â¶</a></h2>
<dl class="class">
<dt id="nlp_architect.models.transformers.token_classification.BertTokenClassificationHead">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.token_classification.</code><code class="descname">BertTokenClassificationHead</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#BertTokenClassificationHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.BertTokenClassificationHead" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertForTokenClassification</span></code></p>
<p>BERT token classification head with linear classifier.
This headâs forward ignores word piece tokens in its linear layer.</p>
<p>The forward requires an additional âvalid_idsâ map that maps the tensors
for valid tokens (e.g., ignores additional word piece tokens generated by
the tokenizer, as in NER task the âXâ label).</p>
<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.BertTokenClassificationHead.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids</em>, <em>token_type_ids=None</em>, <em>attention_mask=None</em>, <em>labels=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>valid_ids=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#BertTokenClassificationHead.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.BertTokenClassificationHead.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.token_classification.QuantizedBertForTokenClassificationHead">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.token_classification.</code><code class="descname">QuantizedBertForTokenClassificationHead</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#QuantizedBertForTokenClassificationHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.QuantizedBertForTokenClassificationHead" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.quantized_bert.QuantizedBertForTokenClassification" title="nlp_architect.models.transformers.quantized_bert.QuantizedBertForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.quantized_bert.QuantizedBertForTokenClassification</span></code></a></p>
<p>Quantized BERT token classification head with linear classifier.
This headâs forward ignores word piece tokens in its linear layer.</p>
<p>The forward requires an additional âvalid_idsâ map that maps the tensors
for valid tokens (e.g., ignores additional word piece tokens generated by
the tokenizer, as in NER task the âXâ label).</p>
<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.QuantizedBertForTokenClassificationHead.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids</em>, <em>token_type_ids=None</em>, <em>attention_mask=None</em>, <em>labels=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>valid_ids=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#QuantizedBertForTokenClassificationHead.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.QuantizedBertForTokenClassificationHead.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.token_classification.</code><code class="descname">RobertaForTokenClassificationHead</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#RobertaForTokenClassificationHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_bert.BertPreTrainedModel</span></code></p>
<p>RoBERTa token classification head with linear classifier.
This headâs forward ignores word piece tokens in its linear layer.</p>
<p>The forward requires an additional âvalid_idsâ map that maps the tensors
for valid tokens (e.g., ignores additional word piece tokens generated by
the tokenizer, as in NER task the âXâ label).</p>
<dl class="attribute">
<dt id="nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.base_model_prefix">
<code class="descname">base_model_prefix</code><em class="property"> = 'roberta'</em><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.base_model_prefix" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.config_class">
<code class="descname">config_class</code><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.config_class" title="Permalink to this definition">Â¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.configuration_roberta.RobertaConfig</span></code></p>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids</em>, <em>attention_mask=None</em>, <em>token_type_ids=None</em>, <em>position_ids=None</em>, <em>head_mask=None</em>, <em>labels=None</em>, <em>valid_ids=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#RobertaForTokenClassificationHead.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.pretrained_model_archive_map">
<code class="descname">pretrained_model_archive_map</code><em class="property"> = {'distilroberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-pytorch_model.bin', 'roberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin', 'roberta-base-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-openai-detector-pytorch_model.bin', 'roberta-large': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin', 'roberta-large-mnli': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-pytorch_model.bin', 'roberta-large-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-openai-detector-pytorch_model.bin'}</em><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead.pretrained_model_archive_map" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.token_classification.</code><code class="descname">TransformerTokenClassifier</code><span class="sig-paren">(</span><em>model_type: str</em>, <em>labels: List[str] = None</em>, <em>*args</em>, <em>load_quantized=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#TransformerTokenClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nlp_architect.models.transformers.base_model.TransformerBase" title="nlp_architect.models.transformers.base_model.TransformerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">nlp_architect.models.transformers.base_model.TransformerBase</span></code></a></p>
<p>Transformer word tagging classifier</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>model_type</strong> (<em>str</em>) â model family (classifier head), choose between bert/quant_bert/xlnet</li>
<li><strong>labels</strong> (<em>List</em><em>[</em><em>str</em><em>]</em><em>, </em><em>optional</em>) â list of tag labels</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.MODEL_CLASS">
<code class="descname">MODEL_CLASS</code><em class="property"> = {'bert': &lt;class 'nlp_architect.models.transformers.token_classification.BertTokenClassificationHead'&gt;, 'quant_bert': &lt;class 'nlp_architect.models.transformers.token_classification.QuantizedBertForTokenClassificationHead'&gt;, 'roberta': &lt;class 'nlp_architect.models.transformers.token_classification.RobertaForTokenClassificationHead'&gt;, 'xlnet': &lt;class 'nlp_architect.models.transformers.token_classification.XLNetTokenClassificationHead'&gt;}</em><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.MODEL_CLASS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.convert_to_tensors">
<code class="descname">convert_to_tensors</code><span class="sig-paren">(</span><em>examples: List[nlp_architect.data.sequential_tagging.TokenClsInputExample], max_seq_length: int = 128, include_labels: bool = True</em><span class="sig-paren">)</span> &#x2192; torch.utils.data.dataset.TensorDataset<a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#TransformerTokenClassifier.convert_to_tensors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.convert_to_tensors" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Convert examples to tensor dataset</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>examples</strong> (<em>List</em><em>[</em><a class="reference internal" href="nlp_architect.data.html#nlp_architect.data.sequence_classification.SequenceClsInputExample" title="nlp_architect.data.sequence_classification.SequenceClsInputExample"><em>SequenceClsInputExample</em></a><em>]</em>) â examples</li>
<li><strong>max_seq_length</strong> (<em>int</em><em>, </em><em>optional</em>) â max sequence length. Defaults to 128.</li>
<li><strong>include_labels</strong> (<em>bool</em><em>, </em><em>optional</em>) â include labels. Defaults to True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">TensorDataset</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.evaluate_predictions">
<code class="descname">evaluate_predictions</code><span class="sig-paren">(</span><em>logits</em>, <em>label_ids</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#TransformerTokenClassifier.evaluate_predictions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.evaluate_predictions" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Run evaluation of given logist and truth labels</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>logits</strong> â model logits</li>
<li><strong>label_ids</strong> â truth label ids</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.extract_labels">
<em class="property">static </em><code class="descname">extract_labels</code><span class="sig-paren">(</span><em>label_ids</em>, <em>label_map</em>, <em>logits</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#TransformerTokenClassifier.extract_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.extract_labels" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><em>examples: List[nlp_architect.data.sequential_tagging.TokenClsInputExample], max_seq_length: int, batch_size: int = 64</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#TransformerTokenClassifier.inference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.inference" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Run inference on given examples</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>examples</strong> (<em>List</em><em>[</em><a class="reference internal" href="nlp_architect.data.html#nlp_architect.data.sequence_classification.SequenceClsInputExample" title="nlp_architect.data.sequence_classification.SequenceClsInputExample"><em>SequenceClsInputExample</em></a><em>]</em>) â examples</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) â batch size. Defaults to 64.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">logits</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>train_data_set: torch.utils.data.dataloader.DataLoader</em>, <em>dev_data_set: Union[torch.utils.data.dataloader.DataLoader</em>, <em>List[torch.utils.data.dataloader.DataLoader]] = None</em>, <em>test_data_set: Union[torch.utils.data.dataloader.DataLoader</em>, <em>List[torch.utils.data.dataloader.DataLoader]] = None</em>, <em>gradient_accumulation_steps: int = 1</em>, <em>per_gpu_train_batch_size: int = 8</em>, <em>max_steps: int = -1</em>, <em>num_train_epochs: int = 3</em>, <em>max_grad_norm: float = 1.0</em>, <em>logging_steps: int = 50</em>, <em>save_steps: int = 100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#TransformerTokenClassifier.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.TransformerTokenClassifier.train" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Run model training</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>train_data_set</strong> (<em>DataLoader</em>) â training dataset</li>
<li><strong>dev_data_set</strong> (<em>Union</em><em>[</em><em>DataLoader</em><em>, </em><em>List</em><em>[</em><em>DataLoader</em><em>]</em><em>]</em><em>, </em><em>optional</em>) â development data set</li>
<li><strong>be list</strong><strong>)</strong><strong> Defaults to None.</strong> (<em>(</em><em>can</em>) â </li>
<li><strong>test_data_set</strong> (<em>Union</em><em>[</em><em>DataLoader</em><em>, </em><em>List</em><em>[</em><em>DataLoader</em><em>]</em><em>]</em><em>, </em><em>optional</em>) â test data set</li>
<li><strong>be list</strong><strong>)</strong><strong> Defaults to None.</strong> â </li>
<li><strong>gradient_accumulation_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â gradient accumulation steps.</li>
<li><strong>to 1.</strong> (<em>Defaults</em>) â </li>
<li><strong>per_gpu_train_batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) â per GPU train batch size (or GPU).</li>
<li><strong>to 8.</strong> (<em>Defaults</em>) â </li>
<li><strong>max_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â max steps for training. Defaults to -1.</li>
<li><strong>num_train_epochs</strong> (<em>int</em><em>, </em><em>optional</em>) â number of training epochs. Defaults to 3.</li>
<li><strong>max_grad_norm</strong> (<em>float</em><em>, </em><em>optional</em>) â max gradient norm. Defaults to 1.0.</li>
<li><strong>logging_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â number of steps between logging. Defaults to 50.</li>
<li><strong>save_steps</strong> (<em>int</em><em>, </em><em>optional</em>) â number of steps between model save. Defaults to 100.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nlp_architect.models.transformers.token_classification.XLNetTokenClassificationHead">
<em class="property">class </em><code class="descclassname">nlp_architect.models.transformers.token_classification.</code><code class="descname">XLNetTokenClassificationHead</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#XLNetTokenClassificationHead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.XLNetTokenClassificationHead" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.modeling_xlnet.XLNetPreTrainedModel</span></code></p>
<p>XLNet token classification head with linear classifier.
This headâs forward ignores word piece tokens in its linear layer.</p>
<p>The forward requires an additional âvalid_idsâ map that maps the tensors
for valid tokens (e.g., ignores additional word piece tokens generated by
the tokenizer, as in NER task the âXâ label).</p>
<dl class="method">
<dt id="nlp_architect.models.transformers.token_classification.XLNetTokenClassificationHead.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input_ids</em>, <em>token_type_ids=None</em>, <em>input_mask=None</em>, <em>attention_mask=None</em>, <em>mems=None</em>, <em>perm_mask=None</em>, <em>target_mapping=None</em>, <em>labels=None</em>, <em>head_mask=None</em>, <em>valid_ids=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/transformers/token_classification.html#XLNetTokenClassificationHead.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.transformers.token_classification.XLNetTokenClassificationHead.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nlp_architect.models.transformers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nlp_architect.models.transformers" title="Permalink to this headline">Â¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>