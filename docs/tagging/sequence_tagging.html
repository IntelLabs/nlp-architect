

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Neural Models for Sequence Tagging &mdash; NLP Architect by Intel® AI Lab 0.5.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/install.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nlp_arch_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:100,900" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sentiment Analysis" href="../sentiment.html" />
    <link rel="prev" title="NLP Architect Model Zoo" href="../model_zoo.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start.html">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Jupyter Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo.html">Model Zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">NLP/NLU Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Sequence Tagging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example">Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#models">Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#word-chunker">Word Chunker</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset">Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-modalities">Running Modalities</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference">Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#named-entity-recognition">Named Entity Recognition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neuraltagger"><code class="docutils literal notranslate"><span class="pre">NeuralTagger</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#cnnlstm"><code class="docutils literal notranslate"><span class="pre">CNNLSTM</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#idcnn"><code class="docutils literal notranslate"><span class="pre">IDCNN</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#transformertokenclassifier"><code class="docutils literal notranslate"><span class="pre">TransformerTokenClassifier</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sentiment.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bist_parser.html">Dependency Parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intent.html">Intent Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lm.html">Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../information_extraction.html">Information Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../archived/additional.html">Additional Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Optimized Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantized_bert.html">Quantized BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transformers_distillation.html">Transformers Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_gnmt.html">Sparse Neural Machine Translation</a></li>
</ul>
<p class="caption"><span class="caption-text">Solutions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../absa_solution.html">Aspect Based Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../term_set_expansion.html">Set Expansion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trend_analysis.html">Trend Analysis</a></li>
</ul>
<p class="caption"><span class="caption-text">For Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../generated_api/nlp_architect_api_index.html">nlp_architect API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide.html">Developer Guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NLP Architect by Intel® AI Lab</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Neural Models for Sequence Tagging</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="neural-models-for-sequence-tagging">
<h1>Neural Models for Sequence Tagging<a class="headerlink" href="#neural-models-for-sequence-tagging" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Token tagging is a core Information extraction task in which words (or phrases) are classified using a pre-defined label set.
Common core NLP tagging tasks are Word Chunking, Part-of-speech (POS) tagging or Named entity recognition (NER).</p>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>Named Entity Recognition (NER) is a basic Information extraction task in which words (or phrases) are classified into pre-defined entity groups (or marked as non interesting). Entity groups share common characteristics of consisting words or phrases and are identifiable by the shape of the word or context in which they appear in sentences. Examples of entity groups are: names, numbers, locations, currency, dates, company names, etc.</p>
<p>Example sentence:</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">John</span> <span class="ow">is</span> <span class="n">planning</span> <span class="n">a</span> <span class="n">visit</span> <span class="n">to</span> <span class="n">London</span> <span class="n">on</span> <span class="n">October</span>
<span class="o">|</span>                           <span class="o">|</span>         <span class="o">|</span>
<span class="n">Name</span>                        <span class="n">City</span>      <span class="n">Date</span>
</pre></div>
</div>
<p>In this example, a <code class="docutils literal notranslate"><span class="pre">name</span></code>, <code class="docutils literal notranslate"><span class="pre">city</span></code> and <code class="docutils literal notranslate"><span class="pre">date</span></code> entities are identified.</p>
</div>
<div class="section" id="models">
<h3>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h3>
<p>NLP Architect includes the following models:</p>
<ul class="simple">
<li>Word Chunking</li>
<li>POS Tagging</li>
<li>Named Entity Recognition</li>
</ul>
</div>
</div>
<div class="section" id="word-chunker">
<h2>Word Chunker<a class="headerlink" href="#word-chunker" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Overview<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Phrase chunking is a basic NLP task that consists of tagging parts of a sentence (1 or more tokens)
syntactically, i.e. POS tagging.</p>
<div class="code bash highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">quick</span> <span class="n">brown</span> <span class="n">fox</span> <span class="n">jumped</span> <span class="n">over</span> <span class="n">the</span> <span class="n">fence</span>
<span class="o">|</span>                   <span class="o">|</span>      <span class="o">|</span>    <span class="o">|</span>
<span class="n">Noun</span>                <span class="n">Verb</span>   <span class="n">Prep</span> <span class="n">Noun</span>
</pre></div>
</div>
<p>In this example the sentence can be divided into 4 phrases, <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">quick</span> <span class="pre">brown</span> <span class="pre">fox</span></code> and <code class="docutils literal notranslate"><span class="pre">the</span> <span class="pre">fence</span></code>
are noun phrases, <code class="docutils literal notranslate"><span class="pre">jumped</span></code> is a verb phrase and <code class="docutils literal notranslate"><span class="pre">over</span></code> is a prepositional phrase.</p>
</div>
<div class="section" id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h3>
<p>We used the <a class="reference external" href="https://www.clips.uantwerpen.be/conll2000/chunking/">CONLL2000</a> shared task dataset in our example for training a phrase chunker. More info about the <a class="reference external" href="https://www.clips.uantwerpen.be/conll2000/chunking/">CONLL2000</a> shared task can be found here: <a class="reference external" href="https://www.clips.uantwerpen.be/conll2000/chunking/">https://www.clips.uantwerpen.be/conll2000/chunking/</a>. The terms and conditions of the data set license apply. Intel does not grant any rights to the data files. The annotation of the data has been derived from the WSJ corpus by a program written by Sabine Buchholz from Tilburg University, The Netherlands.</p>
<p>The <a class="reference external" href="https://www.clips.uantwerpen.be/conll2000/chunking/">CONLL2000</a> dataset has a <code class="docutils literal notranslate"><span class="pre">train_set</span></code> and <code class="docutils literal notranslate"><span class="pre">test_set</span></code> sets consisting of 8926 and 2009 sentences annotated with Part-of-speech and chunking information.
We implemented a dataset loader, <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.CONLL2000" title="nlp_architect.data.sequential_tagging.CONLL2000"><code class="xref py py-class docutils literal notranslate"><span class="pre">CONLL2000</span></code></a>, for loading and parsing <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.CONLL2000" title="nlp_architect.data.sequential_tagging.CONLL2000"><code class="xref py py-class docutils literal notranslate"><span class="pre">CONLL2000</span></code></a> data into numpy arrays ready to be used sequential tagging models. For full set of options please see <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.CONLL2000" title="nlp_architect.data.sequential_tagging.CONLL2000"><code class="xref py py-class docutils literal notranslate"><span class="pre">CONLL2000</span></code></a>.</p>
<p>NLP Architect has a data loader to easily load CONLL2000 which can be found in <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.CONLL2000" title="nlp_architect.data.sequential_tagging.CONLL2000"><code class="xref py py-class docutils literal notranslate"><span class="pre">CONLL2000</span></code></a>. The loader supports the following feature generation when loading the dataset:</p>
<ol class="arabic simple">
<li>Sentence words in sparse int representation</li>
<li>Part-of-speech tags of words</li>
<li>Chunk tag of words (IOB format)</li>
<li>Characters of sentence words in sparse int representation (optional)</li>
</ol>
<p>To get the dataset follow these steps:</p>
<ol class="arabic simple">
<li>download train and test files from dataset website.</li>
<li>unzip files: <code class="docutils literal notranslate"><span class="pre">gunzip</span> <span class="pre">*.gz</span></code></li>
<li>provide <code class="docutils literal notranslate"><span class="pre">CONLL2000</span></code> data loader or <code class="docutils literal notranslate"><span class="pre">train.py</span></code> sample below the directory containing the files.</li>
</ol>
</div>
<div class="section" id="model">
<h3>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h3>
<p>The sequence chunker is a Tensorflow-keras based model and it is implemented in <a class="reference internal" href="../generated_api/nlp_architect.models.html#nlp_architect.models.chunker.SequenceChunker" title="nlp_architect.models.chunker.SequenceChunker"><code class="xref py py-class docutils literal notranslate"><span class="pre">SequenceChunker</span></code></a> and comes with several options for creating the topology depending on what input is given (tokens, external word embedding model, topology parameters).</p>
<p>The model is based on the paper: <a class="reference external" href="http://anthology.aclweb.org/P16-2038">Deep multi-task learning with low level tasks supervised at lower layers</a> by Søgaard and Goldberg (2016), but with minor alterations.</p>
<p>The described model in the paper consists of multiple sequential Bi-directional LSTM layers which are set to predict different tags. the Part-of-speech tags are projected onto a fully connected layer and label tagging is done after the first LSTM layer. The chunk labels are predicted similarly after the 3rd LSTM layer.</p>
<p>The model has additional improvements to the model presented in the paper:</p>
<ul class="simple">
<li>Choose between Conditional Random Fields (<a class="reference internal" href="../generated_api/nlp_architect.nn.tensorflow.python.keras.layers.html#nlp_architect.nn.tensorflow.python.keras.layers.crf.CRF" title="nlp_architect.nn.tensorflow.python.keras.layers.crf.CRF"><code class="xref py py-class docutils literal notranslate"><span class="pre">CRF</span></code></a>) classifier instead of ‘softmax’ as the prediction layers. (models using CRF have been empirically shown to produce more accurate predictions)</li>
<li>Character embeddings using CNNs extracting 3-grams - extracting character information out of words was shown to help syntactic tasks such as tagging and chunking.</li>
</ul>
<p>The model’s embedding vector size and LSTM layer hidden state have equal sizes, the default training optimizer is Adam with default parameters.</p>
</div>
<div class="section" id="running-modalities">
<h3>Running Modalities<a class="headerlink" href="#running-modalities" title="Permalink to this headline">¶</a></h3>
<p>We provide a simple example for training and running inference using the <a class="reference internal" href="../generated_api/nlp_architect.models.html#nlp_architect.models.chunker.SequenceChunker" title="nlp_architect.models.chunker.SequenceChunker"><code class="xref py py-class docutils literal notranslate"><span class="pre">SequenceChunker</span></code></a> model.</p>
<p><code class="docutils literal notranslate"><span class="pre">examples/chunker/train.py</span></code> will load CONLL2000 dataset and train a model using given training parameters (batch size, epochs, external word embedding, etc.), save the model once done training and print the performance of the model on the test set. The example supports loading GloVe/Fasttext word embedding models to be used when training a model. The training method used in this example trains on both POS and Chunk labels concurrently with equal target loss weights, this is different than what is described in the <a class="reference external" href="http://anthology.aclweb.org/P16-2038">paper</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">examples/chunker/inference.py</span></code> will load a saved model and a given text file with sentences and print the chunks found on the stdout.</p>
<div class="section" id="training">
<h4>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h4>
<div class="section" id="quick-train">
<h5>Quick train<a class="headerlink" href="#quick-train" title="Permalink to this headline">¶</a></h5>
<p>Train a model with default parameters (use sentence words and default network settings):</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">examples</span><span class="o">/</span><span class="n">chunker</span><span class="o">/</span><span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">data_dir</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="n">CONLL2000</span> <span class="n">files</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="custom-training-parameters">
<h5>Custom training parameters<a class="headerlink" href="#custom-training-parameters" title="Permalink to this headline">¶</a></h5>
<p>All customizable parameters can be obtained by running: <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train.py</span> <span class="pre">-h</span></code></p>
<table class="docutils option-list" frame="void" rules="none">
<col class="option" />
<col class="description" />
<tbody valign="top">
<tr><td class="option-group">
<kbd><span class="option">-h</span>, <span class="option">--help</span></kbd></td>
<td>show this help message and exit</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--data_dir <var>DATA_DIR</var></span></kbd></td>
</tr>
<tr><td>&#160;</td><td>Path to directory containing CONLL2000 files</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--embedding_model <var>EMBEDDING_MODEL</var></span></kbd></td>
</tr>
<tr><td>&#160;</td><td>Word embedding model path (GloVe/Fasttext/textual)</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--sentence_length <var>SENTENCE_LENGTH</var></span></kbd></td>
</tr>
<tr><td>&#160;</td><td>Maximum sentence length</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--char_features</span></kbd></td>
</tr>
<tr><td>&#160;</td><td>use word character features in addition to words</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--max_word_length <var>MAX_WORD_LENGTH</var></span></kbd></td>
</tr>
<tr><td>&#160;</td><td>maximum number of character in one word (if
–char_features is enabled)</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--feature_size <var>FEATURE_SIZE</var></span></kbd></td>
</tr>
<tr><td>&#160;</td><td>Feature vector size (in embedding and LSTM layers)</td></tr>
<tr><td class="option-group">
<kbd><span class="option">--use_cudnn</span></kbd></td>
<td>use CUDNN based LSTM cells</td></tr>
</tbody>
</table>
<dl class="docutils">
<dt>–classifier {crf,softmax}</dt>
<dd>classifier to use in last layer</dd>
</dl>
<table class="docutils option-list" frame="void" rules="none">
<col class="option" />
<col class="description" />
<tbody valign="top">
<tr><td class="option-group">
<kbd><span class="option">-b <var>B</var></span></kbd></td>
<td>batch size</td></tr>
<tr><td class="option-group">
<kbd><span class="option">-e <var>E</var></span></kbd></td>
<td>number of epochs run fit model</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--model_name <var>MODEL_NAME</var></span></kbd></td>
</tr>
<tr><td>&#160;</td><td>Model name (used for saving the model)</td></tr>
</tbody>
</table>
<p>Saving the model after training is done automatically by specifying a model name with the keyword <cite>–model_name</cite>, the following files will be created:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">chunker_model.h5</span></code> - model file</li>
<li><code class="docutils literal notranslate"><span class="pre">chunker_model.params</span></code> - model parameter files (topology parameters, vocabs)</li>
</ul>
</div>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>Running inference on a trained model using an input file (text based, each line is a document):</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">examples</span><span class="o">/</span><span class="n">chunker</span><span class="o">/</span><span class="n">inference</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name</span> <span class="o">&lt;</span><span class="n">model_name</span><span class="o">&gt;</span> <span class="o">--</span><span class="nb">input</span> <span class="o">&lt;</span><span class="n">input_file</span><span class="o">&gt;.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="named-entity-recognition">
<h2>Named Entity Recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h2>
<div class="section" id="neuraltagger">
<h3><code class="docutils literal notranslate"><span class="pre">NeuralTagger</span></code><a class="headerlink" href="#neuraltagger" title="Permalink to this headline">¶</a></h3>
<p>A model for training token tagging tasks, such as NER or POS. <code class="docutils literal notranslate"><span class="pre">NeuralTagger</span></code> requires an <strong>embedder</strong> for
extracting the contextual features of the data, see embedders below.
The model uses either a <em>Softmax</em> or a <em>Conditional Random Field</em> classifier to classify the words into
correct labels. Implemented in PyTorch and support only PyTorch based embedders.</p>
<p>See <a class="reference internal" href="#nlp_architect.models.tagging.NeuralTagger" title="nlp_architect.models.tagging.NeuralTagger"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralTagger</span></code></a> for complete documentation of model methods.</p>
<dl class="class">
<dt id="nlp_architect.models.tagging.NeuralTagger">
<em class="property">class </em><code class="descclassname">nlp_architect.models.tagging.</code><code class="descname">NeuralTagger</code><span class="sig-paren">(</span><em>embedder_model</em>, <em>word_vocab: nlp_architect.utils.text.Vocabulary</em>, <em>labels: List[str] = None</em>, <em>use_crf: bool = False</em>, <em>device: str = 'cpu'</em>, <em>n_gpus=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/models/tagging.html#NeuralTagger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.models.tagging.NeuralTagger" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple neural tagging model
Supports pytorch embedder models, multi-gpu training, KD from teacher models</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embedder_model</strong> – pytorch embedder model (valid nn.Module model)</li>
<li><strong>word_vocab</strong> (<a class="reference internal" href="../generated_api/nlp_architect.utils.html#nlp_architect.utils.text.Vocabulary" title="nlp_architect.utils.text.Vocabulary"><em>Vocabulary</em></a>) – word vocabulary</li>
<li><strong>labels</strong> (<em>List</em><em>, </em><em>optional</em>) – list of labels. Defaults to None</li>
<li><strong>use_crf</strong> (<em>bool</em><em>, </em><em>optional</em>) – use CRF a the classifier (instead of Softmax). Defaults to False.</li>
<li><strong>device</strong> (<em>str</em><em>, </em><em>optional</em>) – device backend. Defatuls to ‘cpu’.</li>
<li><strong>n_gpus</strong> (<em>int</em><em>, </em><em>optional</em>) – number of gpus. Default to 0.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="cnnlstm">
<h3><code class="docutils literal notranslate"><span class="pre">CNNLSTM</span></code><a class="headerlink" href="#cnnlstm" title="Permalink to this headline">¶</a></h3>
<p>This module is a embedder based on <a class="reference external" href="https://arxiv.org/abs/1603.01354">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a> by Ma and Hovy (2016).
The model uses CNNs to embed character representation of words in a sentence and stacked bi-direction LSTM layers to embed the context of words and characters.</p>
<div class="figure" id="id3">
<img alt="../_images/cnn-lstm-fig.png" src="../_images/cnn-lstm-fig.png" />
<p class="caption"><span class="caption-text">CNN-LSTM topology (taken from original paper)</span></p>
</div>
<p><strong>Usage</strong></p>
<p>Use <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.TokenClsProcessor" title="nlp_architect.data.sequential_tagging.TokenClsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenClsProcessor</span></code></a> for parsing input files for the model. <a class="reference internal" href="#nlp_architect.models.tagging.NeuralTagger" title="nlp_architect.models.tagging.NeuralTagger"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralTagger</span></code></a> for training/loading a trained model.</p>
<p>Training a model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">-</span><span class="n">train</span> <span class="n">tagger</span> <span class="o">--</span><span class="n">model_type</span> <span class="n">cnn</span><span class="o">-</span><span class="n">lstm</span> <span class="o">--</span><span class="n">data_dir</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="n">data</span> <span class="nb">dir</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">model</span> <span class="n">output</span> <span class="nb">dir</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">`nlp-train</span> <span class="pre">tagger</span> <span class="pre">-h`</span></code> for full list of options for training.</p>
<p>Running inference on trained model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">-</span><span class="n">inference</span> <span class="n">tagger</span> <span class="o">--</span><span class="n">data_file</span> <span class="o">&lt;</span><span class="nb">input</span> <span class="n">data</span> <span class="n">file</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">model_dir</span> <span class="o">&lt;</span><span class="n">model</span> <span class="nb">dir</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">output</span> <span class="nb">dir</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">`nlp-inference</span> <span class="pre">tagger</span> <span class="pre">-h`</span></code> for full list of options for running a trained model.</p>
<dl class="class">
<dt id="nlp_architect.nn.torch.modules.embedders.CNNLSTM">
<em class="property">class </em><code class="descclassname">nlp_architect.nn.torch.modules.embedders.</code><code class="descname">CNNLSTM</code><span class="sig-paren">(</span><em>word_vocab_size: int</em>, <em>num_labels: int</em>, <em>word_embedding_dims: int = 100</em>, <em>char_embedding_dims: int = 16</em>, <em>cnn_kernel_size: int = 3</em>, <em>cnn_num_filters: int = 128</em>, <em>lstm_hidden_size: int = 100</em>, <em>lstm_layers: int = 2</em>, <em>bidir: bool = True</em>, <em>dropout: float = 0.5</em>, <em>padding_idx: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/nn/torch/modules/embedders.html#CNNLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.nn.torch.modules.embedders.CNNLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>CNN-LSTM embedder (based on Ma and Hovy. 2016)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>word_vocab_size</strong> (<em>int</em>) – word vocabulary size</li>
<li><strong>num_labels</strong> (<em>int</em>) – number of labels (classifier)</li>
<li><strong>word_embedding_dims</strong> (<em>int</em><em>, </em><em>optional</em>) – word embedding dims</li>
<li><strong>char_embedding_dims</strong> (<em>int</em><em>, </em><em>optional</em>) – character embedding dims</li>
<li><strong>cnn_kernel_size</strong> (<em>int</em><em>, </em><em>optional</em>) – character CNN kernel size</li>
<li><strong>cnn_num_filters</strong> (<em>int</em><em>, </em><em>optional</em>) – character CNN number of filters</li>
<li><strong>lstm_hidden_size</strong> (<em>int</em><em>, </em><em>optional</em>) – LSTM embedder hidden size</li>
<li><strong>lstm_layers</strong> (<em>int</em><em>, </em><em>optional</em>) – num of LSTM layers</li>
<li><strong>bidir</strong> (<em>bool</em><em>, </em><em>optional</em>) – apply bi-directional LSTM</li>
<li><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – dropout rate</li>
<li><strong>padding_idx</strong> (<em>int</em><em>, </em><em>optinal</em>) – padding number for embedding layers</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="idcnn">
<h3><code class="docutils literal notranslate"><span class="pre">IDCNN</span></code><a class="headerlink" href="#idcnn" title="Permalink to this headline">¶</a></h3>
<p>The module is an embedder based on <a class="reference external" href="https://arxiv.org/abs/1702.02098">Fast and Accurate Entity Recognition with Iterated Dilated Convolutions</a> by Strubell et at (2017).
The model uses Iterated-Dilated convolusions for sequence labelling. An dilated CNN block utilizes CNN and dilations to catpure the context of a whole sentence and relation ships between words.
In the figure below you can see an example for a dilated CNN block with maximum dilation of 4 and filter width of 3.
This model is a fast alternative to LSTM-based models with ~10x speedup compared to LSTM-based models.</p>
<div class="figure" id="id4">
<img alt="../_images/idcnn-fig.png" src="../_images/idcnn-fig.png" />
<p class="caption"><span class="caption-text">A dilated CNN block (taken from original paper)</span></p>
</div>
<p>We added a word character convolution feature extractor which is concatenated to the embedded word representations.</p>
<p><strong>Usage</strong></p>
<p>Use <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.TokenClsProcessor" title="nlp_architect.data.sequential_tagging.TokenClsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenClsProcessor</span></code></a> for parsing input files for the model. <a class="reference internal" href="#nlp_architect.models.tagging.NeuralTagger" title="nlp_architect.models.tagging.NeuralTagger"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralTagger</span></code></a> for training/loading a trained model.</p>
<p>Training a model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">-</span><span class="n">train</span> <span class="n">tagger</span> <span class="o">--</span><span class="n">model_type</span> <span class="nb">id</span><span class="o">-</span><span class="n">cnn</span> <span class="o">--</span><span class="n">data_dir</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="n">data</span> <span class="nb">dir</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">model</span> <span class="n">output</span> <span class="nb">dir</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">`nlp-train</span> <span class="pre">tagger</span> <span class="pre">-h`</span></code> for full list of options for training.</p>
<p>Running inference on trained model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">-</span><span class="n">inference</span> <span class="n">tagger</span> <span class="o">--</span><span class="n">data_file</span> <span class="o">&lt;</span><span class="nb">input</span> <span class="n">data</span> <span class="n">file</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">model_dir</span> <span class="o">&lt;</span><span class="n">model</span> <span class="nb">dir</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">output</span> <span class="nb">dir</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">`nlp-inference</span> <span class="pre">tagger</span> <span class="pre">-h`</span></code> for full list of options for running a trained model.</p>
<dl class="class">
<dt id="nlp_architect.nn.torch.modules.embedders.IDCNN">
<em class="property">class </em><code class="descclassname">nlp_architect.nn.torch.modules.embedders.</code><code class="descname">IDCNN</code><span class="sig-paren">(</span><em>word_vocab_size: int</em>, <em>num_labels: int</em>, <em>word_embedding_dims: int = 100</em>, <em>char_embedding_dims: int = 16</em>, <em>char_cnn_filters: int = 128</em>, <em>char_cnn_kernel_size: int = 3</em>, <em>cnn_kernel_size: int = 3</em>, <em>cnn_num_filters: int = 128</em>, <em>input_dropout: float = 0.5</em>, <em>word_dropout: float = 0.5</em>, <em>hidden_dropout: float = 0.5</em>, <em>blocks: int = 1</em>, <em>dilations: List[T] = None</em>, <em>padding_idx: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nlp_architect/nn/torch/modules/embedders.html#IDCNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nlp_architect.nn.torch.modules.embedders.IDCNN" title="Permalink to this definition">¶</a></dt>
<dd><p>ID-CNN (iterated dilated) tagging model (based on Strubell et al 2017) with word character
embedding (using CNN feature extractors)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>word_vocab_size</strong> (<em>int</em>) – word vocabulary size</li>
<li><strong>num_labels</strong> (<em>int</em>) – number of labels (classifier)</li>
<li><strong>word_embedding_dims</strong> (<em>int</em><em>, </em><em>optional</em>) – word embedding dims</li>
<li><strong>char_embedding_dims</strong> (<em>int</em><em>, </em><em>optional</em>) – character embedding dims</li>
<li><strong>char_cnn_filters</strong> (<em>int</em><em>, </em><em>optional</em>) – character CNN kernel size</li>
<li><strong>char_cnn_kernel_size</strong> (<em>int</em><em>, </em><em>optional</em>) – character CNN number of filters</li>
<li><strong>cnn_kernel_size</strong> (<em>int</em><em>, </em><em>optional</em>) – CNN embedder kernel size</li>
<li><strong>cnn_num_filters</strong> (<em>int</em><em>, </em><em>optional</em>) – CNN embedder number of filters</li>
<li><strong>input_dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – input dropout rate</li>
<li><strong>word_dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – pre embedder dropout rate</li>
<li><strong>hidden_dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – pre classifier dropout rate</li>
<li><strong>blocks</strong> (<em>int</em><em>, </em><em>optinal</em>) – number of blocks</li>
<li><strong>dilations</strong> (<em>List</em><em>, </em><em>optinal</em>) – List of dilations per CNN layer</li>
<li><strong>padding_idx</strong> (<em>int</em><em>, </em><em>optinal</em>) – padding number for embedding layers</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="transformertokenclassifier">
<span id="transformer-cls"></span><h3><code class="docutils literal notranslate"><span class="pre">TransformerTokenClassifier</span></code><a class="headerlink" href="#transformertokenclassifier" title="Permalink to this headline">¶</a></h3>
<p>A tagger using a Transformer-based topology and a pre-trained model on a large collection of data (usually wikipedia and such).</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerTokenClassifier</span></code> We provide token tagging classifier head module for Transformer-based pre-trained models.
Currently we support BERT/XLNet and quantized BERT base models which utilize a fully-connected layer with <em>Softmax</em> classifier. Tokens which were broken into multiple sub-tokens (using Wordpiece algorithm or such) are ignored. For a complete list of transformer base models run <code class="docutils literal notranslate"><span class="pre">`nlp-train</span> <span class="pre">transformer_token</span> <span class="pre">-h`</span></code> to see a list of models that can be fine-tuned to your task.</p>
<p><strong>Usage</strong></p>
<p>Use <a class="reference internal" href="../generated_api/nlp_architect.data.html#nlp_architect.data.sequential_tagging.TokenClsProcessor" title="nlp_architect.data.sequential_tagging.TokenClsProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TokenClsProcessor</span></code></a> for parsing input files for the model. Depending on which model you choose, the padding and sentence formatting is adjusted to fit the base model you chose.</p>
<p>See model class <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerTokenClassifier</span></code> for usage documentation.</p>
<p>Training a model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">-</span><span class="n">train</span> <span class="n">transformer_token</span> \
    <span class="o">--</span><span class="n">data_dir</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="n">data</span><span class="o">&gt;</span> \
    <span class="o">--</span><span class="n">model_name_or_path</span> <span class="o">&lt;</span><span class="n">name</span> <span class="n">of</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">model</span> <span class="ow">or</span> <span class="n">path</span><span class="o">&gt;</span> \
    <span class="o">--</span><span class="n">model_type</span> <span class="p">[</span><span class="n">bert</span><span class="p">,</span> <span class="n">quant_bert</span><span class="p">,</span> <span class="n">xlnet</span><span class="p">]</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="n">output</span> <span class="nb">dir</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">`nlp-train</span> <span class="pre">transformer_token</span> <span class="pre">-h`</span></code> for full list of options for training.</p>
<p>Running inference on a trained model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">-</span><span class="n">inference</span> <span class="n">transformer_token</span> \
    <span class="o">--</span><span class="n">data_file</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="nb">input</span> <span class="n">file</span><span class="o">&gt;</span> \
    <span class="o">--</span><span class="n">model_path</span> <span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="n">trained</span> <span class="n">model</span><span class="o">&gt;</span> \
    <span class="o">--</span><span class="n">model_type</span> <span class="p">[</span><span class="n">bert</span><span class="p">,</span> <span class="n">quant_bert</span><span class="p">,</span> <span class="n">xlnet</span><span class="p">]</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">output</span> <span class="n">path</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">nlp-inference</span> <span class="pre">tagger</span> <span class="pre">-h</span></code> for full list of options for running a trained model.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>