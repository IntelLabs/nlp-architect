{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4544237-ad87-4745-8f52-d67f3fd04392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import inflect\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "sys.path.insert(0, 'nlp_architect/models/aspect_extraction_with_kg')\n",
    "import absa_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c6a2c-d1a4-4975-8ffa-bfea637d427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'nlp_architect/models/aspect_extraction_with_kg/data/csv/' # includes the re-labeled devices dataset\n",
    "# data_path = 'nlp_architect/models/aspect_extraction_with_kg/data/csv_original/' # use this path for the original devices dataset\n",
    "domains = ['laptops','restaurants','device']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c50e5-2108-4ce8-b5ea-6229d5f54fc7",
   "metadata": {},
   "source": [
    "### Generate seed terms from unlabeled text using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe319ed-5ae9-439e-8f61-281595daad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}\n",
    "tfidf = {}\n",
    "for target in domains:\n",
    "    docs[target] = []\n",
    "    source = [i for i in domains if i != target][0]\n",
    "    folders = [source + '_to_' + target + '_1', source + '_to_' + target + '_2', source + '_to_' + target + '_3']\n",
    "    for folder in folders:\n",
    "        examples = absa_utils.read_examples_from_file(data_dir=(data_path + folder), mode='test')\n",
    "        nouns = [' '.join([i.words[w] for w in range(len(i.words)) if i.pos_tags[w] in ['NN', 'NNS', 'NNP', 'compound']]).lower() for i in examples]\n",
    "        docs[target] = docs[target] + nouns\n",
    "\n",
    "all_docs = []\n",
    "for target in docs.keys():\n",
    "    all_docs += docs[target]\n",
    "    \n",
    "vectorizer = TfidfVectorizer(lowercase = True, stop_words = 'english', ngram_range = (1, 1), binary=False)\n",
    "X = vectorizer.fit_transform(all_docs)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "tfidf[domains[0]] = df.iloc[:len(docs[domains[0]])].agg('mean').sort_values(ascending=False)\n",
    "tfidf[domains[1]] = df.iloc[len(docs[domains[0]]):(len(docs[domains[0]]) + len(docs[domains[1]]))].agg('mean').sort_values(ascending=False)\n",
    "tfidf[domains[2]] = df.iloc[-len(docs[domains[2]]):].agg('mean').sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d95b9-b226-40f4-9118-d12f9838970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_tokens = list(set(tfidf['device'][:11].index).intersection(tfidf['restaurants'][:11].index).intersection(tfidf['device'][:11].index))\n",
    "for target in domains:\n",
    "    tfidf[target] = tfidf[target].loc[~tfidf[target].index.isin(remove_tokens)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471dd97-49c9-4748-890d-d57c830cffcb",
   "metadata": {},
   "source": [
    "### Query domain-specific KGs from ConceptNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfd415-b815-4415-979b-504e382b83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_edges(query_id, depth, max_edges, rel_types, cn_url, s):\n",
    "    edges = []\n",
    "    for rel in rel_types:\n",
    "        query_url = os.path.join(cn_url, 'query?node=' + query_id + '&other=/c/en')\n",
    "        if rel is not None:\n",
    "            query_url = query_url + '&rel=' + rel\n",
    "        response = s.get(query_url).json()\n",
    "        edges = edges + response['edges']\n",
    "        page = 1\n",
    "        while 'view' in response.keys() and 'nextPage' in response['view'].keys() and 20*page < max_edges:\n",
    "            response = s.get(cn_url + response['view']['nextPage']).json()\n",
    "            edges = edges + response['edges']\n",
    "            page += 1\n",
    "    for edge in edges:\n",
    "        edge['depth'] = depth\n",
    "    edges = [e for e in edges if e['rel']['label'] != 'ExternalURL']\n",
    "    return edges\n",
    "\n",
    "def query_relatedness(word1, word2, cn_url, s):\n",
    "    response = s.get(os.path.join(cn_url, 'relatedness?node1=/c/en/' + word1 + '&node2=/c/en/' + word2)).json()\n",
    "    if 'value' in response.keys():\n",
    "        score = response['value']\n",
    "    else:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def query_subgraph(seed_terms, max_edges, max_depth, min_relatedness, rel_types, cn_url):\n",
    "    seed_dist = []\n",
    "    seed_edges = []\n",
    "    \n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=5,\n",
    "                backoff_factor=0.1,\n",
    "                status_forcelist=[ 500, 502, 503, 504 ])\n",
    "    s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    for seed in tqdm(seed_terms):\n",
    "        edges = query_edges('/c/en/' + seed, 1, max_edges, rel_types, cn_url, s)\n",
    "        n = 0\n",
    "        node_dist = {k: [] for k in range(1, max_depth+1)}\n",
    "        node_edges = {k: [] for k in range(1, max_depth+1)}\n",
    "        while len(edges) > 0:\n",
    "            edge = edges.pop(0)\n",
    "            for node in ['start', 'end']:\n",
    "                token = edge[node]['label']\n",
    "                if token != seed and (('language' not in edge[node].keys()) or (edge[node]['language'] == 'en')) and token not in node_dist[edge['depth']]:\n",
    "                    r_score = query_relatedness(token, seed, cn_url, s)\n",
    "                    if r_score >= min_relatedness or edge['depth'] == 1:\n",
    "                        node_dist[edge['depth']].append(token)\n",
    "                        node_edges[edge['depth']].append(edge)\n",
    "                        if edge['depth'] + 1 <= max_depth:\n",
    "                            add_edges = query_edges(edge[node]['@id'], edge['depth'] + 1, max_edges, rel_types, cn_url, s)\n",
    "                            edges = edges + add_edges\n",
    "            n+=1\n",
    "        seed_dist.append(node_dist)\n",
    "        seed_edges.append(node_edges)\n",
    "        print(str(datetime.now()) + '\\t' + seed + '\\texplored ' + str(n) + ' edges at a max depth of ' + str(max_depth))\n",
    "        \n",
    "    return seed_dist, seed_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b65a0e-96ac-4fde-a2b5-4938616df9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_url = cn_url = 'http://api.conceptnet.io'\n",
    "max_edges = float('inf')\n",
    "max_depth = 2\n",
    "min_relatedness = 0.2\n",
    "rel_types = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388fc14-e8ea-4e46-b8d9-e809de594560",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = Parallel(n_jobs=3)(delayed(query_subgraph)(seed_terms, max_edges, max_depth, min_relatedness, rel_types, cn_url) for seed_terms in [list(tfidf[domain].index) for domain in domains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435b7ea-31fc-4439-87b5-1a50b1dc6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dist = {}; seed_edges = {}\n",
    "for i in range(len(domains)):\n",
    "    seed_dist[domains[i]], seed_edges[domains[i]] = result_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5e7a0-1d86-4524-a7b8-f99776d62c48",
   "metadata": {},
   "source": [
    "### Augment ConceptNet subgraph with generations from COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27853fae-3810-4a68-aa17-bef2d4d0d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comet_results(comet_path):\n",
    "    comet_results = pickle.load(open(comet_path, 'rb'))\n",
    "    comet_results = pd.DataFrame(comet_results)\n",
    "    comet_candidates = []\n",
    "    for i in comet_results['beams']:\n",
    "        comet_candidates += i\n",
    "    comet_candidates = [i for i in comet_candidates if i not in nltk.corpus.stopwords.words('english')]\n",
    "    return comet_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf59b2e-96fc-4c39-a8a4-021b67f45147",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_candidates_laptops = load_comet_results('comet-generations/laptops.pickle')\n",
    "seed_dist['laptops'][0][1] = list(set(seed_dist['laptops'][0][1] + comet_candidates_laptops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518120f-fff2-4cf1-b18f-508c9e28b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_candidates_device = load_comet_results('comet-generations/device.pickle')\n",
    "seed_dist['device'][0][1] = list(set(seed_dist['device'][0][1] + comet_candidates_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61f23a-64da-4821-9367-4c4edbee54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_candidates_restaurants = load_comet_results('comet-generations/restaurants.pickle')\n",
    "seed_dist['restaurants'][0][1] = list(set(seed_dist['restaurants'][0][1] + comet_candidates_restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22668da-77d8-45cb-a7d3-eb0da7f5ebde",
   "metadata": {},
   "source": [
    "### Add plural forms to knowledge graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45613c-1da1-460b-b737-a1a4861ccd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a31762-47e1-4a9b-836b-76af5e77c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pluralize_kg(seed_dist):\n",
    "    for i in range(len(seed_dist)):\n",
    "        for k in seed_dist[i].keys():\n",
    "            for j in range(len(seed_dist[i][k])):\n",
    "                tk = seed_dist[i][k][j].split()\n",
    "                if len(tk) == 1:\n",
    "                    add_tk = engine.plural(tk[0])\n",
    "                else:\n",
    "                    add_tk = ' '.join(tk[:-1] + [engine.plural(tk[-1])])\n",
    "                seed_dist[i][k] += [add_tk]\n",
    "    return seed_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f6563-6f9d-433d-b192-ea4dac6f4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in seed_dist.keys():\n",
    "    seed_dist[domain] = pluralize_kg(seed_dist[domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f69020-2d7c-4e34-bfa7-54621fa03717",
   "metadata": {},
   "source": [
    "### Save domain-specific KGs and seed terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33276fc2-ddcb-483d-8d1b-6692f2089327",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({k : list(tfidf[k].index) for k in tfidf.keys()}, open('seed_terms.pkl', 'wb'))\n",
    "pickle.dump(seed_dist, open('seed_dist.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
