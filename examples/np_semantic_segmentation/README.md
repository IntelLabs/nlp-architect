# Noun Phrase (NP) Semantic Segmentation

Noun-Phrase (NP) is a phrase which has a noun (or pronoun) as its head and zero of more dependent modifiers.
Noun-Phrase is the most frequently occurring phrase type and its inner segmentation is critical for understanding the
semantics of the Noun-Phrase.

This model is the first step in the Semantic Segmentation algorithm - the MLP classifier.
The Semantic Segmentation algorithm takes the dependency relations between the Noun-Phrase words, and the MLP classifier inference as the
input - and build a semantic hierarchy that represents the semantic meaning.
The Semantic Segmentation algorithm eventually create a tree where each tier represent a semantic meaning -> if a sequence of words is a
collocation then a collocation tier is created, else the elements are broken down and each one is mapped
to different tier in the tree.

This model trains MLP classifier and inference from such classifier in order to conclude the correct segmentation
for the given NP.

## Dataset
The expected dataset is a CSV file with 2 columns. the first column contains the Noun-Phrase string (a Noun-Phrase containing 2 words), and the second column contains the correct label (if the 2 word Noun-Phrase is a collocation - the label is 1, else 0)

If you wish to use an existing dataset for training the model, you can download Tratz 2011 et al. dataset [1,2] from the following link:
[Tratz 2011 Dataset](https://vered1986.github.io/papers/Tratz2011_Dataset.tar.gz). Is also available in [here](https://www.isi.edu/publications/licensed-sw/fanseparser/index.html).
(The terms and conditions of the data set license apply. Intel does not grant any rights to the data files or database. see relevant [license agreement](http://www.apache.org/licenses/LICENSE-2.0))


After downloading and unzipping the dataset, run `preprocess_tratz2011.py` in order to construct the labeled data and save it in a CSV file (as expected for the model).
the scripts read 2 .tsv files ('tratz2011_coarse_grained_random/train.tsv' and 'tratz2011_coarse_grained_random/val.tsv') and outputs 2 .csv files accordingly.

Parameters can be obtained by running:

    python preprocess_tratz2011.py -h
        --data path_to_Tratz_2011_dataset_folder


### Pre-processing the data:
A feature vector is extracted from each Noun-Phrase string using the command `python data.py`

* Word2Vec word embedding (300 size vector for each word in the Noun-Phrase) .
    * Pre-trained Google News Word2vec model can download [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)
    * The terms and conditions of the data set license apply. Intel does not grant any rights to the data files or database. see relevant [license agreement](http://www.apache.org/licenses/LICENSE-2.0)
* Cosine distance between 2 words in the Noun-Phrase.
* NLTKCollocations score (NPMI and UCI scores).
* A binary features whether the Noun-Phrase has existing entity in Wikidata.
* A binary features whether the Noun-Phrase has existing entity in WordNet.

Quick example:

    python data.py --data input_data_path.csv --output output_prepared_path.csv --w2v_path <path_to_w2v>/GoogleNews-vectors-negative300.bin.gz

## Training
The command `python train.py` will train the MLP classifier and evaluate it.
After training is done, the model is saved automatically:

Quick example:

    python train.py --data prepared_data_path.csv --model_path np_semantic_segmentation_path.prm

## Inference
In order to run inference you need to have pre-trained `<model_name>.prm` file and data CSV file
that was generated by `prepare_data.py`.
The result of `python inference.py` is a CSV file, each row contains the model's inference in respect to the input data.

Quick example:

    python inference.py --model np_semantic_segmentation_path.prm --data prepared_data_path.csv --output inference_data.csv --print_stats
